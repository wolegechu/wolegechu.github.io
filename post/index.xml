<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yuchu Luo - 罗宇矗</title>
    <link>/post/</link>
    <description>Recent content in Posts on Yuchu Luo - 罗宇矗</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Yuchu Luo</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Virtual Memory Hardware</title>
      <link>/post/vm_hardware/</link>
      <pubDate>Tue, 26 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/vm_hardware/</guid>
      <description>

&lt;h3 id=&#34;issues-in-sharing-physical-memory&#34;&gt;Issues in sharing physical memory&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-152306.png&#34; alt=&#34;&#34; /&gt;
- Protection
    - A bug in one process can corrupt in another
    - Must somehow prevent process A from trashing B&amp;rsquo;s memory
    - Also prevent A from even observing B&amp;rsquo;s memory (ssh-agent)
- Transparency
    - A process shouldn&amp;rsquo;t require particular physical memory bits
    - Yet processes often require large amounts of contiguous memory (for stack, large data structures, etc.)
- Resource exhaustion
    - Programmers typically assume machine has &amp;ldquo;enough&amp;rdquo; memory
    - Sum of sizes of all processes often greater than physical memory&lt;/p&gt;

&lt;h2 id=&#34;virtual-memory&#34;&gt;Virtual memory&lt;/h2&gt;

&lt;h3 id=&#34;goals&#34;&gt;goals&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Give each program its own &lt;em&gt;virtual&lt;/em&gt; address space

&lt;ul&gt;
&lt;li&gt;At runtime, &lt;strong&gt;Memory-Management Unit (MMU)&lt;/strong&gt; relocates each load/store&lt;/li&gt;
&lt;li&gt;Application doesn&amp;rsquo;t see physical memory addresses&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Also enforce protection

&lt;ul&gt;
&lt;li&gt;Prevent one app from messing with another&amp;rsquo;s memory&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And allow programs to see more memory than exists

&lt;ul&gt;
&lt;li&gt;Somehow relocate some memory accesses to disk&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;virtual-memory-advantages&#34;&gt;Virtual memory advantages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Can relocate program while running

&lt;ul&gt;
&lt;li&gt;Run partially in memory, partially on disk&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Most of a process&amp;rsquo;s memory may be idle (&lt;sup&gt;80&lt;/sup&gt;&amp;frasl;&lt;sub&gt;20&lt;/sub&gt; rule)

&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;/img/post/WX20171226-153927.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;li&gt;Write idle parts to disk until needed&lt;/li&gt;
&lt;li&gt;Let other processes use memory of idle part&lt;/li&gt;
&lt;li&gt;Like CPU virtualization: when process not using CPU, switch (Not using amemory region? switch it to another process)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Challenge: VM = extra layer, could be slow&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ideas&#34;&gt;Ideas&lt;/h2&gt;

&lt;h3 id=&#34;ideal-1-load-time-linking&#34;&gt;Ideal 1: load-time linking&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-154928.png&#34; alt=&#34;&#34; /&gt;
- &lt;em&gt;Linker&lt;/em&gt; patches addresses of symbols like printf
- Idea: link when process executed, not at compile time
    - Determine where process will reside in memory
    - Adjust all refernces within program (using addtion)
- Problems?
    - How to enforce protection?
    - How to move once already in memory? (consider data pointers)
    - What if no contiguous free region fits program?&lt;/p&gt;

&lt;h3 id=&#34;ideal-2-base-bound-register&#34;&gt;Ideal 2: base + bound register&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-163437.png&#34; alt=&#34;&#34; /&gt;
- Two special privileged registers: &lt;strong&gt;base&lt;/strong&gt; snf &lt;strong&gt;bound&lt;/strong&gt;
- On each load/store/jump:
    - Physical address = virtual address + base
    - Check 0 ≤ vitual address ≤ bound, else trap to kernel
- How to move process in memory?
    - Change base register
- What happens on context switch?
    - OS must re-load base and bound register&lt;/p&gt;

&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Programs load/store to &lt;code&gt;virtual addresses&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Actual memory uses &lt;code&gt;physical addresses&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;VM Hardware is Memory Management Unit (&lt;code&gt;MMU&lt;/code&gt;)

&lt;ul&gt;
&lt;li&gt;Usually part of CPU&lt;/li&gt;
&lt;li&gt;Configured through privileged instructions (e.g., load bound reg)&lt;/li&gt;
&lt;li&gt;Translates from virtual to physical addresses&lt;/li&gt;
&lt;li&gt;Gives per-process view of memory called &lt;code&gt;address space&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;base-bound&#34;&gt;Base + bound&lt;/h2&gt;

&lt;h3 id=&#34;trade-offs&#34;&gt;Trade-offs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Advantages

&lt;ul&gt;
&lt;li&gt;Cheap in terms of hardware: only two registers&lt;/li&gt;
&lt;li&gt;Cheap in terms of cycles: do add and compare in parallel&lt;/li&gt;
&lt;li&gt;Examples: Cray-1 used this scheme&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disadvantages

&lt;ul&gt;
&lt;li&gt;Growing a process is expensive or impossible&lt;/li&gt;
&lt;li&gt;No way to share code or data (E.g., two copies of bochs, both running pintos)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One solution: Multiple segments

&lt;ul&gt;
&lt;li&gt;E.g., separate code, stack, data segments&lt;/li&gt;
&lt;li&gt;Possibly multiple data segments&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;segmentation&#34;&gt;Segmentation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-164915.png&#34; alt=&#34;&#34; /&gt;
- Let processes have many base/bound regs
    - Address space built from many segments
    - Can share/protect memory at segment granularity
- Must specify segment as part of virtual address&lt;/p&gt;

&lt;h3 id=&#34;segmentation-mechanics&#34;&gt;Segmentation mechanics&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-191449.png&#34; alt=&#34;&#34; /&gt;
- Each process has a segment table
- Each VA (Virtual Address) indicates a segment and offset:
    - Top bits of addr select segment, low bits select offset (PDP-10)
    - Or segment selected by instruction or operand (means you need wider &amp;ldquo;far&amp;rdquo; pointers to specify segment)&lt;/p&gt;

&lt;h3 id=&#34;segmentation-example&#34;&gt;Segmentation example&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-192145.png&#34; alt=&#34;&#34; /&gt;
- 2-bit segment number (1st digit), 12 bit offset (last 3)
    - Where is 0x0&lt;/p&gt;

&lt;h3 id=&#34;trade-offs-1&#34;&gt;Trade-offs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Advantages

&lt;ul&gt;
&lt;li&gt;Multiple segments per process&lt;/li&gt;
&lt;li&gt;Allows sharing!&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t need entire process in memory&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disadvantages

&lt;ul&gt;
&lt;li&gt;Requires translation hardware, which could limit performance&lt;/li&gt;
&lt;li&gt;Segments not completely transparent to program (e.g., default segment faster or uses shorter instruction)&lt;/li&gt;
&lt;li&gt;$n$ byte segment needs &lt;em&gt;n contiguous&lt;/em&gt; bytes of physical memory&lt;/li&gt;
&lt;li&gt;Makes &lt;em&gt;fragmentation&lt;/em&gt; a real problem&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;fragmentation&#34;&gt;Fragmentation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Fragmentation =&amp;gt; Inability to use free memory&lt;/li&gt;
&lt;li&gt;Over time:

&lt;ul&gt;
&lt;li&gt;Variable-sized pices = many small holes (external fragmentation)&lt;/li&gt;
&lt;li&gt;Fixed-sized pieces = no external holes, but force internal waste (internal fragmentation)
&lt;img src=&#34;/img/post/WX20171226-213655.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;alternatives-to-hardware-mmu&#34;&gt;Alternatives to hardware MMU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Language-level protection (Java)

&lt;ul&gt;
&lt;li&gt;Single address space for different modules&lt;/li&gt;
&lt;li&gt;Language enforces isolation&lt;/li&gt;
&lt;li&gt;Singularity OS does this&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Software fault isolation

&lt;ul&gt;
&lt;li&gt;Instrument compiler output&lt;/li&gt;
&lt;li&gt;Checks before ever ystore operation prevents modules from trashing each other&lt;/li&gt;
&lt;li&gt;Google Native Client does this&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;paging&#34;&gt;Paging&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Divede memory up into small &lt;em&gt;pages&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Map virtual pages to physical pages

&lt;ul&gt;
&lt;li&gt;Each process has separate mapping&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Allow OS to gain control on certain operation

&lt;ul&gt;
&lt;li&gt;Read-only pages trap to OS on write&lt;/li&gt;
&lt;li&gt;Invalid pages trap to OS on read or write&lt;/li&gt;
&lt;li&gt;OS can change mapping and resume application&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Other features sometimes found:

&lt;ul&gt;
&lt;li&gt;Hardware can set &amp;ldquo;accessed&amp;rdquo; and &amp;ldquo;dirty&amp;rdquo; bits&lt;/li&gt;
&lt;li&gt;Control page execute permission separately from read/write&lt;/li&gt;
&lt;li&gt;Control caching or memory consistency of page&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;trade-offs-2&#34;&gt;Trade-offs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Eliminates external fragmentation&lt;/li&gt;
&lt;li&gt;Simplifies allocation, free, and backing storage (swap)&lt;/li&gt;
&lt;li&gt;Average internal fragmentation of .5 pages per &amp;ldquo;segment&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;simplified-allocation&#34;&gt;Simplified allocation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Allocate any physical page to any process&lt;/li&gt;
&lt;li&gt;Can store idle virtual pages on disk&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;paging-data-structures&#34;&gt;Paging data structures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Pages are fixed size, e.g., 4K

&lt;ul&gt;
&lt;li&gt;Least significant 12 ($log_{2}4K$) bits of address are page offset&lt;/li&gt;
&lt;li&gt;Most significant bits are &lt;em&gt;page number&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Each process has a page table

&lt;ul&gt;
&lt;li&gt;Maps &lt;em&gt;virtual page numbers (VPNs)&lt;/em&gt; to &lt;em&gt;physical page numbers (PPNS)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Also includes bits for protection, validity, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;On memory access: Translate VPN to PPN, then add offset
&lt;img src=&#34;/img/post/WX20171227-094654.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;###Example: Paging on PDP-11
- 64K virtual memory, 8K pages
    - Separate address space for instructions &amp;amp; data
    - I.e., can&amp;rsquo;t read your own instructions with a load
- Entire page table stored in registers
    - 8 Instruction page translation
    - 8 Data page translation
- Swap 16 machine registers on each context switch&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-12</title>
      <link>/post/til-2017-12-12/</link>
      <pubDate>Tue, 12 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-12/</guid>
      <description>

&lt;h2 id=&#34;note-deep-learning-practice-and-trends-nips-2017&#34;&gt;[Note] Deep Learning: Practice and Trends (NIPS 2017)&lt;/h2&gt;

&lt;h3 id=&#34;practice&#34;&gt;Practice&lt;/h3&gt;

&lt;h4 id=&#34;architectures&#34;&gt;Architectures&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Convolutional Nets

&lt;ul&gt;
&lt;li&gt;Locality: objects tend to have a local spatial support

&lt;ul&gt;
&lt;li&gt;locally-connected&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Teanslation invariance: object appearance is independent of location&lt;/li&gt;
&lt;li&gt;Tricks of the Trade

&lt;ul&gt;
&lt;li&gt;Optimization

&lt;ul&gt;
&lt;li&gt;SGD with momentum&lt;/li&gt;
&lt;li&gt;Batch Norm&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Initialization

&lt;ul&gt;
&lt;li&gt;Weight init: start from the weights which lead to stable training&lt;/li&gt;
&lt;li&gt;Sample from zero-mean normal distribution w/ small variance 0.01

&lt;ul&gt;
&lt;li&gt;Adaptively choose variance for each layer

&lt;ul&gt;
&lt;li&gt;preserve gradient magnitude: 1/sqrt(fan_in)&lt;/li&gt;
&lt;li&gt;works fine for VGGNets (up to 20 layers), but not sufficient for deeper nets&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Model

&lt;ul&gt;
&lt;li&gt;Stacking 3x3 convolutions&lt;/li&gt;
&lt;li&gt;Inception&lt;/li&gt;
&lt;li&gt;ResNet adds modules which ensure that the gradient doesn&amp;rsquo;t vanish&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;recurrent-nets&#34;&gt;Recurrent Nets&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Two Key Ingredients: Neural Embeddings, Recurrent Language Models&lt;/li&gt;
&lt;li&gt;Dot product Attention

&lt;ul&gt;
&lt;li&gt;Inputs: &amp;ldquo;I am a cat&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Input RNN states: $e_1e_2e_3e_4$&lt;/li&gt;
&lt;li&gt;Decoder RNN state at step i (query): $h_i$&lt;/li&gt;
&lt;li&gt;Compute scalars $h_i^Te_1, h_i^Te_2,&amp;hellip;$representing similarity / relevance between encoder steps and query&lt;/li&gt;
&lt;li&gt;Normaliza [h_i^Te_1,h_i^Te_2,&amp;hellip;] with softmax to produce attention weights&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tricks of the Trade

&lt;ul&gt;
&lt;li&gt;Long sequences?

&lt;ul&gt;
&lt;li&gt;Attention&lt;/li&gt;
&lt;li&gt;Bigger state&lt;/li&gt;
&lt;li&gt;Reverse inputs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Can&amp;rsquo;t overfit?

&lt;ul&gt;
&lt;li&gt;Bigger hidden state&lt;/li&gt;
&lt;li&gt;Deep LSTM + Skip Connections&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Overfit?

&lt;ul&gt;
&lt;li&gt;Dropout + Ensembles&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tuning

&lt;ul&gt;
&lt;li&gt;Keep calm and decrease your learning rate&lt;/li&gt;
&lt;li&gt;Initalization of parameters is critical (in seq2seq we used U(-0.05, 0.05))&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clip the gradients!&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;E.g. if ||grad|| &amp;gt; 5: grad = grad/||grad|| * 5&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Attention and Memory Toolbox

&lt;ul&gt;
&lt;li&gt;Read/Write memories (neural turing machine)&lt;/li&gt;
&lt;li&gt;Sequence Prediction&lt;/li&gt;
&lt;li&gt;Seq2Seq&lt;/li&gt;
&lt;li&gt;Temporal Hierarchies&lt;/li&gt;
&lt;li&gt;Multimodality&lt;/li&gt;
&lt;li&gt;Attention/Pointers&lt;/li&gt;
&lt;li&gt;Recurrent Architectures&lt;/li&gt;
&lt;li&gt;Key, Value memories&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;trends&#34;&gt;Trends&lt;/h3&gt;

&lt;h4 id=&#34;autoregressive-models&#34;&gt;Autoregressive Models&lt;/h4&gt;

&lt;p&gt;$$P(x;\theta) = \prod_{n=1}^N P(x&lt;em&gt;n|x&lt;/em&gt;{&lt;n&gt;};\theta)$$
- Each factor can be parametrized by $\theta$, which can be shared
- The variables can be arbitrarily ordered and grouped, as long as the ordering and grouping is consistent.
- Building Blocks
    - Inputs and Outpus: these can also be conditioning variables
        - Image pixels
        - Text sequences
        - Audio waveforms
    - Architectures
        - Recurrent, over space and time
        - Causal convolutions
        - Causal conv + attention
        - Attention-only!
    - Losses:
        - Discrete case: softmax cross entropy
        - Continuous: Gaussian (mixture) likelihood
        - &lt;strong&gt;Mixture of logistics loss&lt;/strong&gt; (PixelCNN++ in ICLR 2017)
            - $$ v ~ \sum_{i=1}^K \pi_i logistic(\mu_i,s&lt;em&gt;i) $$
            - $$ P(x|\pi,\mu,s) = \sum&lt;/em&gt;{i=1}^K \pi_i[\sigma((x+0.5-\mu_i)/s_i) - \sigma((x-0.5-\mu_i) / s_i)] $$
- Autogressive scoring and sampling
    - Fully sequential models:
        - PixelCNN, PixelCNN++, WaveNet, &amp;hellip;
        - O(1) scoring, O(N) sampling
    - Models with conditional independence assumptions:
        - O(1) scoring, sampling can be O(1), O(log N), etc depending on cond. indep. assumptions
    - Distilled models:
        - Parall WaveNet, Parallel NMT
        - O(N) scoring, O(1) sampling&lt;/p&gt;

&lt;h4 id=&#34;domain-alignment-unsupervised&#34;&gt;Domain Alignment (unsupervised)&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Building Blocks

&lt;ul&gt;
&lt;li&gt;Inputs and Outpus

&lt;ul&gt;
&lt;li&gt;Sets of images with shared structure, but weak or no alignment&lt;/li&gt;
&lt;li&gt;Text corpora in different languages, but not parallel&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Architecures

&lt;ul&gt;
&lt;li&gt;Nothing fancy!&lt;/li&gt;
&lt;li&gt;For images: mostly convolutional nets&lt;/li&gt;
&lt;li&gt;For text: recurrent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Losses

&lt;ul&gt;
&lt;li&gt;Latent space: Domain confusion&lt;/li&gt;
&lt;li&gt;Pixel space: Cycle consistency&lt;/li&gt;
&lt;li&gt;Both adversarial loss and likelihoods work!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Approach

&lt;ul&gt;
&lt;li&gt;Cross-modal retrieval&lt;/li&gt;
&lt;li&gt;Unsupervised domain transfer for classification&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Case&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DiscoGAN - Car2Face&lt;/li&gt;
&lt;li&gt;GraspGAN&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Unsupervised Neural Machine Translation&lt;/p&gt;

&lt;h4 id=&#34;learning-to-learn-meta-learning&#34;&gt;Learning to Learn / Meta Learning&lt;/h4&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Building Blocks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Inputs and Outputs: text, images, actions&lt;/li&gt;
&lt;li&gt;Architectures: Recurrent, CNN (+attention)&lt;/li&gt;
&lt;li&gt;Losses (loss based on another loss): Model, Optimization, Initialization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learning to learn:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is Meta Learning?

&lt;ul&gt;
&lt;li&gt;Go beyond train/test from same distribution&lt;/li&gt;
&lt;li&gt;Task between train/test changes, so model has to &amp;ldquo;learn to learn&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model Based, Metric Based, Optimization Based&lt;/p&gt;

&lt;h2 id=&#34;conclusions-and-expectations&#34;&gt;Conclusions and Expectations&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep autoregressive models and ConvNets are ubiquitous and already useful in consumer applications&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Inductive biases are useful&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spatial invariance for CNNs&lt;/li&gt;
&lt;li&gt;time recurrence for RNNs&lt;/li&gt;
&lt;li&gt;Permutation invariance for Graphs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Simple tricks like ResNet will be discovered&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adversarial networks and unsupervised domain adaptation have interesting market app (e.g. phone apps like style transfer)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Meta learning: more and more of the model lifecycle (train/val/test) will be learned in an end-to-end way&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Program syntesis + Graph networks may be very important and find more real-world applications (e.g. RobustFill)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-08</title>
      <link>/post/til-2017-12-08/</link>
      <pubDate>Fri, 08 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-08/</guid>
      <description>

&lt;h2 id=&#34;paper-daily&#34;&gt;Paper Daily&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Where Classification Fails, Interpretation Rises

&lt;ul&gt;
&lt;li&gt;apply an attention mechanism to the adversarial examples detection&lt;/li&gt;
&lt;li&gt;uisng attention to defend against adversarial examples&lt;/li&gt;
&lt;li&gt;(this paper is hard to read)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deep Image Prior

&lt;ul&gt;
&lt;li&gt;randomly-initialized neural network can be used as a handcrafted prior with excellent results&lt;/li&gt;
&lt;li&gt;search in parameter space&lt;/li&gt;
&lt;li&gt;apply untrianed CNN, fit a G network to a single degraded image.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Learning framework for Autonoumous Driving

&lt;ul&gt;
&lt;li&gt;Deep Deterministic Actor Critic (DDAC)

&lt;ul&gt;
&lt;li&gt;actor: provides the policy mapping from a state to action&lt;/li&gt;
&lt;li&gt;critic: evaluates the value of the action taken (same as Q-function)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;review-of-clustering-algorithms&#34;&gt;Review of clustering algorithms&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Clustering

&lt;ul&gt;
&lt;li&gt;k-means&lt;/li&gt;
&lt;li&gt;k-medoids&lt;/li&gt;
&lt;li&gt;Gaussian Mixture Model&lt;/li&gt;
&lt;li&gt;Spectral Clustering&lt;/li&gt;
&lt;li&gt;Hierachical Clustering&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Other relevant algorithms

&lt;ul&gt;
&lt;li&gt;Expecatation Maximization&lt;/li&gt;
&lt;li&gt;Dimendsionality Reduction

&lt;ul&gt;
&lt;li&gt;Laplacian Eigenmap&lt;/li&gt;
&lt;li&gt;Locally Linear Embedding&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;computer-orgnisation&#34;&gt;Computer orgnisation&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Direct mapping

&lt;ul&gt;
&lt;li&gt;process is divided into pages&lt;/li&gt;
&lt;li&gt;main memory is divided into frames/blocks&lt;/li&gt;
&lt;li&gt;cache is divided into lines&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-05</title>
      <link>/post/til-2017-12-05/</link>
      <pubDate>Tue, 05 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-05/</guid>
      <description>

&lt;h2 id=&#34;paper-daily&#34;&gt;Paper Daily&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;(NIPS 2017) Visual Reference Resolution using Attention Memory for Visual Dialog, Paul Hongsuch Seo

&lt;ul&gt;
&lt;li&gt;An associative attention memory storing s sequence of previous (attention, key) pairs&lt;/li&gt;
&lt;li&gt;Retrieves the previous attention, taking into account recency, which is most relevant for the current Q&lt;/li&gt;
&lt;li&gt;Resoning ability, maybe the best single model (trained by mle) presently&lt;/li&gt;
&lt;li&gt;Other keywords, text as key, attention as value, aceess values according to key; Method is Attention over attention&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;computer-network-notes&#34;&gt;Computer Network Notes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Classes (think in binary system)

&lt;ul&gt;
&lt;li&gt;CA: 1-126 ($2^{24}-2$)&lt;/li&gt;
&lt;li&gt;CB: 128-191 ($2^{16}-2$)&lt;/li&gt;
&lt;li&gt;CC: 192-223 ($2^8-2$)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DBA: Directed Broadcast Address (NID, HID 1s [11.255.255.255])&lt;/li&gt;
&lt;li&gt;NID: (NID, HID 0s [11.0.0.0])&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;types-of-casting&#34;&gt;Types of casting&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Unicast&lt;/li&gt;
&lt;li&gt;Broadcast

&lt;ul&gt;
&lt;li&gt;Limited

&lt;ul&gt;
&lt;li&gt;11.0.0.0  |m|11.1.2.3 (Source Address)|255.255.255.255 (Destination Address)|&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Directed

&lt;ul&gt;
&lt;li&gt;11.0.0.0  |m|11.1.2.3|20.255.255.255|&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;subnets-subnet-mask-cidr&#34;&gt;Subnets, Subnet Mask, CIDR&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;subnetting disadvantage

&lt;ul&gt;
&lt;li&gt;reach the process complexly&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The difference between in subnet and outside subnet;&lt;/li&gt;
&lt;li&gt;Every subnet waste two IP addresses for the NID and DBA

&lt;ul&gt;
&lt;li&gt;2 subnet: For CC: (128 - 2) x 2 = 252&lt;/li&gt;
&lt;li&gt;2 subnet: (200.1.2.0)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Subnet Mask: 32 bit

&lt;ul&gt;
&lt;li&gt;#1: #NID + #SID, #0: #HID&lt;/li&gt;
&lt;li&gt;can find out NID&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Variable lenght subnet masking (VLSM)&lt;/li&gt;
&lt;li&gt;Classless Inter Domain Routing

&lt;ul&gt;
&lt;li&gt;(a.b.c.d/n) n: #NID&lt;/li&gt;
&lt;li&gt;Rules of blocks

&lt;ul&gt;
&lt;li&gt;All IP addrees should be contiguous&lt;/li&gt;
&lt;li&gt;$2^n$&lt;/li&gt;
&lt;li&gt;Fast IP address in the block should be evenly divided by size of the block&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Subnetting in CIDR&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;operating-system-notes&#34;&gt;Operating System Notes&lt;/h2&gt;

&lt;h2 id=&#34;file-system&#34;&gt;File System&lt;/h2&gt;

&lt;h3 id=&#34;cache-memory&#34;&gt;cache memory&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Cache - Paging (main memory) - Secondary Memory

&lt;ul&gt;
&lt;li&gt;Hit latency: the time to hit in the cache&lt;/li&gt;
&lt;li&gt;Cache hit: a state in which data requested for processing by a component or application is found in the cache memory&lt;/li&gt;
&lt;li&gt;Cache miss: not found&lt;/li&gt;
&lt;li&gt;Miss latency: the time (in cycles) the CPU waits when a miss happen in the cache&lt;/li&gt;
&lt;li&gt;Page fault, Page hit&lt;/li&gt;
&lt;li&gt;Spatial/temporal locality&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Direct Mapping

&lt;ul&gt;
&lt;li&gt;[Tag | Index (line number)| (block) Offset]&lt;/li&gt;
&lt;li&gt;$2^m$ addresses&lt;/li&gt;
&lt;li&gt;$2^k$ cache entries&lt;/li&gt;
&lt;li&gt;$2^n$ block size&lt;/li&gt;
&lt;li&gt;Step:

&lt;ul&gt;
&lt;li&gt;Use the index part of the address to find the appropriate cache entry&lt;/li&gt;
&lt;li&gt;CHeck the tag to see if the entry contains the right data&lt;/li&gt;
&lt;li&gt;If it does, then use the offset to the find the correct byte&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to Imitation Learning (Part 1)</title>
      <link>/post/imitation-learning-1/</link>
      <pubDate>Sun, 12 Mar 2017 22:38:00 +0000</pubDate>
      
      <guid>/post/imitation-learning-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Knowledge arrangement about Reinforcement Learning</title>
      <link>/post/knowledge-arrangement-about-rl/</link>
      <pubDate>Wed, 20 Apr 2016 11:00:00 +0000</pubDate>
      
      <guid>/post/knowledge-arrangement-about-rl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The knowledge of domain name that everyone should know</title>
      <link>/post/knowledge-of-domain-name/</link>
      <pubDate>Mon, 11 Jan 2016 07:45:00 +0000</pubDate>
      
      <guid>/post/knowledge-of-domain-name/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The construction of user portrait based on Big Data (Theoretical section)</title>
      <link>/post/user-portrait-theoretical/</link>
      <pubDate>Sun, 10 Jan 2016 05:58:00 +0000</pubDate>
      
      <guid>/post/user-portrait-theoretical/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>

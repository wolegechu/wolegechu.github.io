<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yuchu Luo - 罗宇矗 on Yuchu Luo - 罗宇矗</title>
    <link>/</link>
    <description>Recent content in Yuchu Luo - 罗宇矗 on Yuchu Luo - 罗宇矗</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Yuchu Luo</copyright>
    <lastBuildDate>Fri, 02 Jun 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>2017-12-12</title>
      <link>/post/til-2017-12-12/</link>
      <pubDate>Tue, 12 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-12/</guid>
      <description>

&lt;h2 id=&#34;note-deep-learning-practice-and-trends-nips-2017&#34;&gt;[Note] Deep Learning: Practice and Trends (NIPS 2017)&lt;/h2&gt;

&lt;h3 id=&#34;practice&#34;&gt;Practice&lt;/h3&gt;

&lt;h4 id=&#34;architectures&#34;&gt;Architectures&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Convolutional Nets

&lt;ul&gt;
&lt;li&gt;Locality: objects tend to have a local spatial support

&lt;ul&gt;
&lt;li&gt;locally-connected&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Teanslation invariance: object appearance is independent of location&lt;/li&gt;
&lt;li&gt;Tricks of the Trade

&lt;ul&gt;
&lt;li&gt;Optimization

&lt;ul&gt;
&lt;li&gt;SGD with momentum&lt;/li&gt;
&lt;li&gt;Batch Norm&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Initialization

&lt;ul&gt;
&lt;li&gt;Weight init: start from the weights which lead to stable training&lt;/li&gt;
&lt;li&gt;Sample from zero-mean normal distribution w/ small variance 0.01

&lt;ul&gt;
&lt;li&gt;Adaptively choose variance for each layer

&lt;ul&gt;
&lt;li&gt;preserve gradient magnitude: 1/sqrt(fan_in)&lt;/li&gt;
&lt;li&gt;works fine for VGGNets (up to 20 layers), but not sufficient for deeper nets&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Model

&lt;ul&gt;
&lt;li&gt;Stacking 3x3 convolutions&lt;/li&gt;
&lt;li&gt;Inception&lt;/li&gt;
&lt;li&gt;ResNet adds modules which ensure that the gradient doesn&amp;rsquo;t vanish&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;recurrent-nets&#34;&gt;Recurrent Nets&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Two Key Ingredients: Neural Embeddings, Recurrent Language Models&lt;/li&gt;
&lt;li&gt;Dot product Attention

&lt;ul&gt;
&lt;li&gt;Inputs: &amp;ldquo;I am a cat&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Input RNN states: $e_1e_2e_3e_4$&lt;/li&gt;
&lt;li&gt;Decoder RNN state at step i (query): $h_i$&lt;/li&gt;
&lt;li&gt;Compute scalars $h_i^Te_1, h_i^Te_2,&amp;hellip;$representing similarity / relevance between encoder steps and query&lt;/li&gt;
&lt;li&gt;Normaliza [h_i^Te_1,h_i^Te_2,&amp;hellip;] with softmax to produce attention weights&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tricks of the Trade

&lt;ul&gt;
&lt;li&gt;Long sequences?

&lt;ul&gt;
&lt;li&gt;Attention&lt;/li&gt;
&lt;li&gt;Bigger state&lt;/li&gt;
&lt;li&gt;Reverse inputs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Can&amp;rsquo;t overfit?

&lt;ul&gt;
&lt;li&gt;Bigger hidden state&lt;/li&gt;
&lt;li&gt;Deep LSTM + Skip Connections&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Overfit?

&lt;ul&gt;
&lt;li&gt;Dropout + Ensembles&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tuning

&lt;ul&gt;
&lt;li&gt;Keep calm and decrease your learning rate&lt;/li&gt;
&lt;li&gt;Initalization of parameters is critical (in seq2seq we used U(-0.05, 0.05))&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clip the gradients!&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;E.g. if ||grad|| &amp;gt; 5: grad = grad/||grad|| * 5&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Attention and Memory Toolbox

&lt;ul&gt;
&lt;li&gt;Read/Write memories (neural turing machine)&lt;/li&gt;
&lt;li&gt;Sequence Prediction&lt;/li&gt;
&lt;li&gt;Seq2Seq&lt;/li&gt;
&lt;li&gt;Temporal Hierarchies&lt;/li&gt;
&lt;li&gt;Multimodality&lt;/li&gt;
&lt;li&gt;Attention/Pointers&lt;/li&gt;
&lt;li&gt;Recurrent Architectures&lt;/li&gt;
&lt;li&gt;Key, Value memories&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;trends&#34;&gt;Trends&lt;/h3&gt;

&lt;h4 id=&#34;autoregressive-models&#34;&gt;Autoregressive Models&lt;/h4&gt;

&lt;p&gt;$$P(x;\theta) = \prod_{n=1}^N P(x&lt;em&gt;n|x&lt;/em&gt;{&lt;n&gt;};\theta)$$
- Each factor can be parametrized by $\theta$, which can be shared
- The variables can be arbitrarily ordered and grouped, as long as the ordering and grouping is consistent.
- Building Blocks
    - Inputs and Outpus: these can also be conditioning variables
        - Image pixels
        - Text sequences
        - Audio waveforms
    - Architectures
        - Recurrent, over space and time
        - Causal convolutions
        - Causal conv + attention
        - Attention-only!
    - Losses:
        - Discrete case: softmax cross entropy
        - Continuous: Gaussian (mixture) likelihood
        - &lt;strong&gt;Mixture of logistics loss&lt;/strong&gt; (PixelCNN++ in ICLR 2017)
            - $$ v ~ \sum_{i=1}^K \pi_i logistic(\mu_i,s&lt;em&gt;i) $$
            - $$ P(x|\pi,\mu,s) = \sum&lt;/em&gt;{i=1}^K \pi_i[\sigma((x+0.5-\mu_i)/s_i) - \sigma((x-0.5-\mu_i) / s_i)] $$
- Autogressive scoring and sampling
    - Fully sequential models:
        - PixelCNN, PixelCNN++, WaveNet, &amp;hellip;
        - O(1) scoring, O(N) sampling
    - Models with conditional independence assumptions:
        - O(1) scoring, sampling can be O(1), O(log N), etc depending on cond. indep. assumptions
    - Distilled models:
        - Parall WaveNet, Parallel NMT
        - O(N) scoring, O(1) sampling&lt;/p&gt;

&lt;h4 id=&#34;domain-alignment-unsupervised&#34;&gt;Domain Alignment (unsupervised)&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Building Blocks

&lt;ul&gt;
&lt;li&gt;Inputs and Outpus

&lt;ul&gt;
&lt;li&gt;Sets of images with shared structure, but weak or no alignment&lt;/li&gt;
&lt;li&gt;Text corpora in different languages, but not parallel&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Architecures

&lt;ul&gt;
&lt;li&gt;Nothing fancy!&lt;/li&gt;
&lt;li&gt;For images: mostly convolutional nets&lt;/li&gt;
&lt;li&gt;For text: recurrent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Losses

&lt;ul&gt;
&lt;li&gt;Latent space: Domain confusion&lt;/li&gt;
&lt;li&gt;Pixel space: Cycle consistency&lt;/li&gt;
&lt;li&gt;Both adversarial loss and likelihoods work!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Approach

&lt;ul&gt;
&lt;li&gt;Cross-modal retrieval&lt;/li&gt;
&lt;li&gt;Unsupervised domain transfer for classification&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Case&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DiscoGAN - Car2Face&lt;/li&gt;
&lt;li&gt;GraspGAN&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Unsupervised Neural Machine Translation&lt;/p&gt;

&lt;h4 id=&#34;learning-to-learn-meta-learning&#34;&gt;Learning to Learn / Meta Learning&lt;/h4&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Building Blocks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Inputs and Outputs: text, images, actions&lt;/li&gt;
&lt;li&gt;Architectures: Recurrent, CNN (+attention)&lt;/li&gt;
&lt;li&gt;Losses (loss based on another loss): Model, Optimization, Initialization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Learning to learn:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is Meta Learning?

&lt;ul&gt;
&lt;li&gt;Go beyond train/test from same distribution&lt;/li&gt;
&lt;li&gt;Task between train/test changes, so model has to &amp;ldquo;learn to learn&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model Based, Metric Based, Optimization Based&lt;/p&gt;

&lt;h2 id=&#34;conclusions-and-expectations&#34;&gt;Conclusions and Expectations&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep autoregressive models and ConvNets are ubiquitous and already useful in consumer applications&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Inductive biases are useful&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spatial invariance for CNNs&lt;/li&gt;
&lt;li&gt;time recurrence for RNNs&lt;/li&gt;
&lt;li&gt;Permutation invariance for Graphs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Simple tricks like ResNet will be discovered&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adversarial networks and unsupervised domain adaptation have interesting market app (e.g. phone apps like style transfer)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Meta learning: more and more of the model lifecycle (train/val/test) will be learned in an end-to-end way&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Program syntesis + Graph networks may be very important and find more real-world applications (e.g. RobustFill)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-05</title>
      <link>/post/til-2017-12-05/</link>
      <pubDate>Tue, 05 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-05/</guid>
      <description>

&lt;h2 id=&#34;paper-daily&#34;&gt;Paper Daily&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;(NIPS 2017) Visual Reference Resolution using Attention Memory for Visual Dialog, Paul Hongsuch Seo

&lt;ul&gt;
&lt;li&gt;An associative attention memory storing s sequence of previous (attention, key) pairs&lt;/li&gt;
&lt;li&gt;Retrieves the previous attention, taking into account recency, which is most relevant for the current Q&lt;/li&gt;
&lt;li&gt;Resoning ability, maybe the best single model (trained by mle) presently&lt;/li&gt;
&lt;li&gt;Other keywords, text as key, attention as value, aceess values according to key; Method is Attention over attention&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;computer-network-notes&#34;&gt;Computer Network Notes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Classes (think in binary system)

&lt;ul&gt;
&lt;li&gt;CA: 1-126 ($2^{24}-2$)&lt;/li&gt;
&lt;li&gt;CB: 128-191 ($2^{16}-2$)&lt;/li&gt;
&lt;li&gt;CC: 192-223 ($2^8-2$)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DBA: Directed Broadcast Address (NID, HID 1s [11.255.255.255])&lt;/li&gt;
&lt;li&gt;NID: (NID, HID 0s [11.0.0.0])&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;types-of-casting&#34;&gt;Types of casting&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Unicast&lt;/li&gt;
&lt;li&gt;Broadcast

&lt;ul&gt;
&lt;li&gt;Limited

&lt;ul&gt;
&lt;li&gt;11.0.0.0  |m|11.1.2.3 (Source Address)|255.255.255.255 (Destination Address)|&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Directed

&lt;ul&gt;
&lt;li&gt;11.0.0.0  |m|11.1.2.3|20.255.255.255|&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;subnets-subnet-mask-cidr&#34;&gt;Subnets, Subnet Mask, CIDR&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;subnetting disadvantage

&lt;ul&gt;
&lt;li&gt;reach the process complexly&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The difference between in subnet and outside subnet;&lt;/li&gt;
&lt;li&gt;Every subnet waste two IP addresses for the NID and DBA

&lt;ul&gt;
&lt;li&gt;2 subnet: For CC: (128 - 2) x 2 = 252&lt;/li&gt;
&lt;li&gt;2 subnet: (200.1.2.0)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Subnet Mask: 32 bit

&lt;ul&gt;
&lt;li&gt;#1: #NID + #SID, #0: #HID&lt;/li&gt;
&lt;li&gt;can find out NID&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Variable lenght subnet masking (VLSM)&lt;/li&gt;
&lt;li&gt;Classless Inter Domain Routing

&lt;ul&gt;
&lt;li&gt;(a.b.c.d/n) n: #NID&lt;/li&gt;
&lt;li&gt;Rules of blocks

&lt;ul&gt;
&lt;li&gt;All IP addrees should be contiguous&lt;/li&gt;
&lt;li&gt;$2^n$&lt;/li&gt;
&lt;li&gt;Fast IP address in the block should be evenly divided by size of the block&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Subnetting in CIDR&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;operating-system-notes&#34;&gt;Operating System Notes&lt;/h2&gt;

&lt;h2 id=&#34;file-system&#34;&gt;File System&lt;/h2&gt;

&lt;h3 id=&#34;cache-memory&#34;&gt;cache memory&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Cache - Paging (main memory) - Secondary Memory

&lt;ul&gt;
&lt;li&gt;Hit latency: the time to hit in the cache&lt;/li&gt;
&lt;li&gt;Cache hit: a state in which data requested for processing by a component or application is found in the cache memory&lt;/li&gt;
&lt;li&gt;Cache miss: not found&lt;/li&gt;
&lt;li&gt;Miss latency: the time (in cycles) the CPU waits when a miss happen in the cache&lt;/li&gt;
&lt;li&gt;Page fault, Page hit&lt;/li&gt;
&lt;li&gt;Spatial/temporal locality&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Direct Mapping

&lt;ul&gt;
&lt;li&gt;[Tag | Index (line number)| (block) Offset]&lt;/li&gt;
&lt;li&gt;$2^m$ addresses&lt;/li&gt;
&lt;li&gt;$2^k$ cache entries&lt;/li&gt;
&lt;li&gt;$2^n$ block size&lt;/li&gt;
&lt;li&gt;Step:

&lt;ul&gt;
&lt;li&gt;Use the index part of the address to find the appropriate cache entry&lt;/li&gt;
&lt;li&gt;CHeck the tag to see if the entry contains the right data&lt;/li&gt;
&lt;li&gt;If it does, then use the offset to the find the correct byte&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-05</title>
      <link>/post/til-2017-12-06/</link>
      <pubDate>Tue, 05 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-06/</guid>
      <description>

&lt;h2 id=&#34;paper-daily&#34;&gt;Paper Daily&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Where Classification Fails, Interpretation Rises

&lt;ul&gt;
&lt;li&gt;apply an attention mechanism to the adversarial examples detection&lt;/li&gt;
&lt;li&gt;uisng attention to defend against adversarial examples&lt;/li&gt;
&lt;li&gt;(this paper is hard to read)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deep Image Prior

&lt;ul&gt;
&lt;li&gt;randomly-initialized neural network can be used as a handcrafted prior with excellent results&lt;/li&gt;
&lt;li&gt;search in parameter space&lt;/li&gt;
&lt;li&gt;apply untrianed CNN, fit a G network to a single degraded image.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Learning framework for Autonoumous Driving

&lt;ul&gt;
&lt;li&gt;Deep Deterministic Actor Critic (DDAC)

&lt;ul&gt;
&lt;li&gt;actor: provides the policy mapping from a state to action&lt;/li&gt;
&lt;li&gt;critic: evaluates the value of the action taken (same as Q-function)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;review-of-clustering-algorithms&#34;&gt;Review of clustering algorithms&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Clustering

&lt;ul&gt;
&lt;li&gt;k-means&lt;/li&gt;
&lt;li&gt;k-medoids&lt;/li&gt;
&lt;li&gt;Gaussian Mixture Model&lt;/li&gt;
&lt;li&gt;Spectral Clustering&lt;/li&gt;
&lt;li&gt;Hierachical Clustering&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Other relevant algorithms

&lt;ul&gt;
&lt;li&gt;Expecatation Maximization&lt;/li&gt;
&lt;li&gt;Dimendsionality Reduction

&lt;ul&gt;
&lt;li&gt;Laplacian Eigenmap&lt;/li&gt;
&lt;li&gt;Locally Linear Embedding&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;computer-orgnisation&#34;&gt;Computer orgnisation&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Direct mapping

&lt;ul&gt;
&lt;li&gt;process is divided into pages&lt;/li&gt;
&lt;li&gt;main memory is divided into frames/blocks&lt;/li&gt;
&lt;li&gt;cache is divided into lines&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>End-to-end Deep Models For Self-driving Car (Avaliable soon)</title>
      <link>/talk/end-to-end-deep-models-for-self-driving-car/</link>
      <pubDate>Sun, 26 Nov 2017 19:00:00 +0000</pubDate>
      
      <guid>/talk/end-to-end-deep-models-for-self-driving-car/</guid>
      <description>

&lt;hr /&gt;

&lt;h2 id=&#34;reference&#34;&gt;Reference:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Huval, Brody, et al. &amp;ldquo;An empirical evaluation of deep learning on highway driving.&amp;ldquo; arXiv preprint arXiv:1504.01716 (2015).&lt;/li&gt;
&lt;li&gt;Ulbrich, Simon, et al. &amp;ldquo;Towards a Functional System Architecture for Automated Vehicles.&amp;ldquo; arXiv preprint arXiv:1703.08557 (2017).&lt;/li&gt;
&lt;li&gt;Pomerleau, Dean A. &amp;ldquo;Alvinn: An autonomous land vehicle in a neural network.&amp;ldquo; 
Advances in neural information processing systems. 1989.&lt;/li&gt;
&lt;li&gt;Muller, Urs, et al. &amp;ldquo;Off-road obstacle avoidance through end-to-end learning.“ Advances in neural information processing systems. 2006.APA&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Chen, Chenyi, et al. &amp;ldquo;Deepdriving: Learning affordance for direct perception in autonomous driving.&amp;ldquo; Proceedings of the IEEE International Conference on Computer Vision. 2015.&lt;/li&gt;
&lt;li&gt;Chen, Chenyi, et al. &amp;ldquo;Deepdriving: Learning affordance for direct perception in autonomous driving.&amp;ldquo; Proceedings of the IEEE International Conference on Computer Vision. 2015.&lt;/li&gt;
&lt;li&gt;Bojarski, Mariusz, et al. &amp;ldquo;End to end learning for self-driving cars.&amp;ldquo; arXiv preprint arXiv:1604.07316 (2016).&lt;/li&gt;
&lt;li&gt;Codevilla, Felipe, et al. &amp;ldquo;End-to-end driving via conditional imitation learning.&amp;ldquo; arXiv preprint arXiv:1710.02410 (2017).&lt;/li&gt;
&lt;li&gt;Xu, Huazhe, et al. &amp;ldquo;End-to-end learning of driving models from large-scale video datasets.&amp;ldquo; arXiv preprint arXiv:1612.01079(2016).&lt;/li&gt;
&lt;li&gt;Mukadam, Mustafa, et al. “Tactical Decision Making for Lane Changing with Deep Reinforcement Learning.&amp;rdquo; (2017).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>卷积神经网络：从相遇相知到相爱</title>
      <link>/talk/cnn/</link>
      <pubDate>Fri, 10 Nov 2017 19:00:00 +0000</pubDate>
      
      <guid>/talk/cnn/</guid>
      <description>&lt;hr /&gt;

&lt;script async class=&#39;speakerdeck-embed&#39; data-id=&#39;7aba17f26f474cb9aebb0ffba3c11446&#39; data-ratio=&#39;1.33333333333333&#39; src=&#39;//speakerdeck.com/assets/embed.js&#39;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>如何实现无人驾驶</title>
      <link>/talk/how-to-build-self-driving-car/</link>
      <pubDate>Fri, 20 Oct 2017 19:00:00 +0000</pubDate>
      
      <guid>/talk/how-to-build-self-driving-car/</guid>
      <description>&lt;p&gt;MIL 智能体组无人驾驶汽车项目介绍&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning</title>
      <link>/talk/deep-reinforcemnt-learning/</link>
      <pubDate>Mon, 28 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/deep-reinforcemnt-learning/</guid>
      <description>&lt;script async class=&#39;speakerdeck-embed&#39; data-id=&#39;62b49c13db9b4b61a97bc64140d96453&#39; data-ratio=&#39;1.33333333333333&#39; src=&#39;//speakerdeck.com/assets/embed.js&#39;&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;Markov Decision Processes&lt;/li&gt;
&lt;li&gt;Policy Learning and Value Learning&lt;/li&gt;
&lt;li&gt;Bellman Equation&lt;/li&gt;
&lt;li&gt;Model-based and Model-free&lt;/li&gt;
&lt;li&gt;Q-learning, Policy Gradients, Actor-Critic&lt;/li&gt;
&lt;li&gt;Experience Replay&lt;/li&gt;
&lt;li&gt;Applications: Atari Game, Recurrent Attention Model(RAM)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Referfence and Recommend Materials&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CS231n: Deep Reinforcement Learning (&lt;a href=&#34;https://www.youtube.com/watch?v=lvoHnicueoE&amp;amp;index=14&amp;amp;list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&#34; target=&#34;_blank&#34;&gt;Video&lt;/a&gt;) (&lt;a href=&#34;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf&#34; target=&#34;_blank&#34;&gt;Slide&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34; target=&#34;_blank&#34;&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1406.6247&#34; target=&#34;_blank&#34;&gt;Recurrent Models of Visual Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow&#34; target=&#34;_blank&#34;&gt;Reinforcement Learning Methods and Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Reinforcement Learning</title>
      <link>/talk/reinforcement-learning/</link>
      <pubDate>Wed, 23 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/reinforcement-learning/</guid>
      <description>&lt;script async class=&#39;speakerdeck-embed&#39; data-id=&#39;2b0ad1af004f4266b693269e394f35cf&#39; data-ratio=&#39;1.33333333333333&#39; src=&#39;//speakerdeck.com/assets/embed.js&#39;&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;Markov Decision Processes&lt;/li&gt;
&lt;li&gt;Model-based and Model-free&lt;/li&gt;
&lt;li&gt;Value Iteration and Policy Iteration&lt;/li&gt;
&lt;li&gt;Q-learning and approximate Q-learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Referfence and Recommend Materials&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25319023&#34; target=&#34;_blank&#34;&gt;强化学习（Reinforcement Learning）知识整理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;UC Berkeley CS188 Intro to AI&#34; target=&#34;_blank&#34;&gt;UC Berkeley CS188 Intro to AI&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Markov Decision Processe

&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;http://www.youtube.com/watch?v=Oxqwwnm_x0s&#34; target=&#34;_blank&#34;&gt;lecture I&lt;/a&gt;) (&lt;a href=&#34;http://www.youtube.com/watch?v=6pBvbLyn6fE&#34; target=&#34;_blank&#34;&gt;lecture II&lt;/a&gt;) (&lt;a href=&#34;http://ai.berkeley.edu/slides/Lecture%208%20--%20MDPs%20I/SP14%20CS188%20Lecture%208%20--%20MDPs%20I.pptx&#34; target=&#34;_blank&#34;&gt;PPT I&lt;/a&gt;) (&lt;a href=&#34;http://ai.berkeley.edu/slides/Lecture%209%20--%20MDPs%20II/SP14%20CS188%20Lecture%209%20--%20MDPs%20II.pptx&#34; target=&#34;_blank&#34;&gt;PPT II&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Reinforcement Learning

&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;http://www.youtube.com/watch?v=IXuHxkpO5E8&#34; target=&#34;_blank&#34;&gt;lecture I&lt;/a&gt;) (&lt;a href=&#34;http://www.youtube.com/watch?v=yNeSFbE1jdY&#34; target=&#34;_blank&#34;&gt;lecture II&lt;/a&gt;) (&lt;a href=&#34;http://ai.berkeley.edu/slides/Lecture%2010%20--%20Reinforcement%20Learning%20I/SP14%20CS188%20Lecture%2010%20--%20Reinforcement%20Learning%20I.pptx&#34; target=&#34;_blank&#34;&gt;PPT I&lt;/a&gt;) (&lt;a href=&#34;http://ai.berkeley.edu/slides/Lecture%2011%20--%20Reinforcement%20Learning%20II/SP14%20CS188%20Lecture%2011%20--%20Reinforcement%20Learning%20II.pptx&#34; target=&#34;_blank&#34;&gt;PPT II&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>An Quadrotor Safety Monitorting System</title>
      <link>/project/an-quadrotor-safety-monitorting-system/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/an-quadrotor-safety-monitorting-system/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Implemented and optimized a anomaly detection algorithm called Isolation Forest, which can receive temporal information and feed back the abnormal degree.&lt;/li&gt;
&lt;li&gt;Developed an exploration tool for the iForest algorithm, which can do visualization to a certain extent&lt;/li&gt;
&lt;li&gt;Won Provincial 1st Prizes both in the 2017 Challenge Cup and the E-Commerce Competition, Technical group. And get the qualification to the 2017 national contests.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Rossmann Store Sales</title>
      <link>/project/rossmann/</link>
      <pubDate>Sun, 19 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/rossmann/</guid>
      <description>&lt;p&gt;Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.&lt;/p&gt;

&lt;p&gt;In their first Kaggle competition, Rossmann is challenging you to predict 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on what’s most important to them: their customers and their teams!&lt;/p&gt;

&lt;p&gt;(There are a lot of flaws due to lack of time.)
You can see the report &lt;a href=&#34;https://github.com/wolegechu/Machine_Learning_Nanodegree/tree/master/Capstone&#34; target=&#34;_blank&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Digit Recognition Program</title>
      <link>/project/digit_recognition/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/digit_recognition/</guid>
      <description>&lt;p&gt;In this project, I used deep neural networks and convolutional neural networks to create a program that prints numbers it observes in real time from images it is given. First, I designed and tested a model architecture that can identify sequences of digits in an image. Next, I trained that model so it could decode sequences of digits from natural images by using the Street View House Numbers (SVHN) dataset. After the model was properly trained, I then tested my model using a program on newly-captured images. Finally, I refined your implementation to also localize where numbers are on the image, and test this localization on newly-captured images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The main techniques used:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Deep Neural Networks&lt;/li&gt;
&lt;li&gt;Convolutional Neural Networks&lt;/li&gt;
&lt;li&gt;Adadelta&lt;/li&gt;
&lt;li&gt;Keras based on TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the code(iPython notebook) &lt;a href=&#34;https://github.com/wolegechu/Machine_Learning_Nanodegree/blob/master/5.%20Digit%20Recognition/digit_recognition.ipynb&#34; target=&#34;_blank&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Train a Smartcab How to Drive</title>
      <link>/project/smartcab/</link>
      <pubDate>Tue, 14 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/smartcab/</guid>
      <description>

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;In this project I applied reinforcement learning techniques for a self-driving agent in a simplified world to aid it in effectively reaching its destinations in the allotted time. I first investigated the environment the agent operates in by constructing a very basic driving implementation. Once the agent is successful at operating within the environment, I then identified each possible state the agent can be in when considering such things as traffic lights and oncoming traffic at each intersection. With states identified, I then implemented a Q-Learning algorithm for the self-driving agent to guide the agent towards its destination within the allotted time. Finally, I improved upon the Q-Learning algorithm to find the best configuration of learning and exploration factors to ensure the self-driving agent is reaching its destinations with consistently positive results.&lt;/p&gt;

&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;

&lt;p&gt;In the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents, known as &lt;em&gt;smartcabs&lt;/em&gt;, to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to depend on &lt;em&gt;smartcabs&lt;/em&gt; to get to where they need to go as safely and reliably as possible. Although &lt;em&gt;smartcabs&lt;/em&gt; have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or reliable as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, my task is to use reinforcement learning techniques to construct a demonstration of a &lt;em&gt;smartcab&lt;/em&gt; operating in real-time to prove that both safety and reliability can be achieved.&lt;/p&gt;

&lt;h3 id=&#34;below-is-the-final-score&#34;&gt;Below is the final score:&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/posters/smartcab.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-main-techniques-used&#34;&gt;The main techniques used:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Markov Decision Processes&lt;/li&gt;
&lt;li&gt;Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Game Theory&lt;/li&gt;
&lt;li&gt;More Game Theory beginning at Stochastic games and Multi-agent RL&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the code(iPython notebook) &lt;a href=&#34;https://github.com/wolegechu/Machine_Learning_Nanodegree/blob/master/4.%20Smartcab/smartcab.ipynb&#34; target=&#34;_blank&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to Imitation Learning (Part 1)</title>
      <link>/post/imitation-learning-1/</link>
      <pubDate>Sun, 12 Mar 2017 22:38:00 +0000</pubDate>
      
      <guid>/post/imitation-learning-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Creating Customer Segments</title>
      <link>/project/customer-segments/</link>
      <pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/customer-segments/</guid>
      <description>&lt;p&gt;In this project I applied unsupervised learning techniques on product spending data collected for customers of a wholesale distributor in Lisbon, Portugal to identify customer segments hidden in the data. I first explored the data by selecting a small subset to sample and determine if any product categories highly correlate with one another. Afterwards, I preprocessed the data by scaling each product category and then identifying (and removing) unwanted outliers. With the good, clean customer spending data, I applied PCA transformations to the data and implement clustering algorithms to segment the transformed customer data. Finally, I compared the segmentation found with an additional labeling and consider ways this information could assist the wholesale distributor with future service changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The main techniques used:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Clustering&lt;/li&gt;
&lt;li&gt;More Clustering&lt;/li&gt;
&lt;li&gt;Feature Scaling&lt;/li&gt;
&lt;li&gt;Feature Selection&lt;/li&gt;
&lt;li&gt;PCA&lt;/li&gt;
&lt;li&gt;Feature Transformation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the code(iPython notebook) &lt;a href=&#34;https://github.com/wolegechu/Machine_Learning_Nanodegree/blob/master/3.%20Customer%20Segments/customer_segments.ipynb&#34; target=&#34;_blank&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Finding Donors for CharityML</title>
      <link>/project/finding-donors/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/finding-donors/</guid>
      <description>&lt;p&gt;In this project, I applied supervised learning techniques and an analytical mind on data collected for the U.S. census to help CharityML (a fictitious charity organization) identify people most likely to donate to their cause. I first explored the data to learn how the census data is recorded. Next, I applied a series of transformations and preprocessing techniques to manipulate the data into a workable format. Then I evaluated several supervised learners of my choice on the data, and considered which is best suited for the solution. Afterwards, I optimized the model I had selected and presented it as my solution to CharityML. Finally, I explored the chosen model and its predictions under the hood, to see just how well it&amp;rsquo;s performing when considering the data it&amp;rsquo;s given. predicted selling price to the statistics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The main techniques used:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Decision Trees&lt;/li&gt;
&lt;li&gt;Regression &amp;amp; Classification&lt;/li&gt;
&lt;li&gt;Regressions&lt;/li&gt;
&lt;li&gt;Kernel Methods &amp;amp; SVM&lt;/li&gt;
&lt;li&gt;GaussianNB&lt;/li&gt;
&lt;li&gt;Ensemble learning&lt;/li&gt;
&lt;li&gt;RandomForestClassifier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can see the code(iPython notebook) &lt;a href=&#34;https://github.com/wolegechu/Machine_Learning_Nanodegree/blob/master/2.%20Finding%20Donors/finding_donors.ipynb&#34; target=&#34;_blank&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

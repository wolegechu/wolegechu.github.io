<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yuchu Luo - 罗宇矗</title>
    <link>/post/</link>
    <description>Recent content in Posts on Yuchu Luo - 罗宇矗</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Yuchu Luo</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Apollo Planning 规划模块简析（附大致流程）</title>
      <link>/post/apollo_planning/</link>
      <pubDate>Wed, 21 Feb 2018 00:38:00 +0000</pubDate>
      
      <guid>/post/apollo_planning/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;



&lt;p&gt;Apollo 的 Planning 模块以 Routing 模块产生一条当前位置到终点的理想路径作为输入，结合当前场景下的路况信息（信号灯、短期内障碍物的运动轨迹等），生成短期内的一条驾驶路径。&lt;/p&gt;

&lt;p&gt;在展开之前先介绍下 Apollo 主要采用的 Frenet 坐标系，见下图，整个坐标系基于一条光滑的参考线（右图中红线），然后将汽车的坐投影到参考线上，得到一个参考线上的投影点（图中蓝色点）。从参考线起点到投影点的路径长度就是汽车在 Frenet 坐标系下的纵向偏移量，用 S 表示。而投影点到汽车位置的距离则是汽车在Frenet坐标系下的横向偏移量，用 L 表示。通过汽车的朝向、速度、加速度也可以计算出 Frenet 坐标系下，横向和纵向偏移量的一阶导和二阶导。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/apollo_frenet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Planning 模块的大致流程如下图&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzI1NjkxOTMyNQ==&amp;amp;mid=2247485310&amp;amp;idx=1&amp;amp;sn=830f95c8fe1c0fe300ab4baa735361d5&amp;amp;chksm=ea1e150cdd699c1a149879f5d044b6ed866d0ae5d30c59aa571cdbcad2cad738c964c397fcdd&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0914fHtF0ADvSZWvZ7WSioeu&amp;amp;rd2werd=1#wechat_redirect，首先使用&#34; target=&#34;_blank&#34;&gt;https://mp.weixin.qq.com/s?__biz=MzI1NjkxOTMyNQ==&amp;amp;mid=2247485310&amp;amp;idx=1&amp;amp;sn=830f95c8fe1c0fe300ab4baa735361d5&amp;amp;chksm=ea1e150cdd699c1a149879f5d044b6ed866d0ae5d30c59aa571cdbcad2cad738c964c397fcdd&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0914fHtF0ADvSZWvZ7WSioeu&amp;amp;rd2werd=1#wechat_redirect，首先使用&lt;/a&gt; Routing 模块返回的全局路径通过 Planning and Control (PnC) Map 生成多条未经平滑的参考线，范围为车辆后方 30m 至前方 150～250m，经过平滑后交由 Frame 类处理。Frame 类初始化使用 PnC Map 生成的参考线，然后融合从感知模块获取的障碍物信息 PathObstacle 和障碍物（以及交规）对路径的决策影响 PathDecision 构成 ReferenceLineInfo 类。同时基于参考线信息和其中障碍物未来时刻的轨迹信息，规划器（EM planner 或 Lattice planner）会生成一条未来短距离内（如 40m）的最优规划路径 PathData (S-L 序列)，以及短时间内（如 8s, 150m）的速度规划序列 SpeedData (S-T 序列)，然后将两者融合得到最优路径 （S-L-T序列）交由 Control 模块处理。&lt;/p&gt;

&lt;p&gt;PnC Map 主要做的事是生成参考线，这里参考线可以直接看作是车道中心线，通常会对当前车道和当前车道可驶入的邻接车道（该邻接车道也需存在于全局规划的路径中）分别生成参考线供后续模块使用。&lt;/p&gt;

&lt;p&gt;在 Frame 类中，因为 PnC Map 给出的参考线实质上是一系列的点，所以会首先对每条参考线进行光滑处理，这里也涉及到了跨车道（如路口、分流、汇流等）情况下的参考线拼接。然后从感知模块获取障碍物信息进行滞后预测（lagged prediction），即除了使用 Prediction 模块发布的最新一次信息，还会考虑障碍物的历史轨迹预测数据以供更精确地预测。再将障碍物的轨迹信息加入到参考线中，并判断每个障碍物对自身轨迹规划产生的决策影响（纵向：忽略，停车，超车，减速，跟车；横向：忽略，微调，绕行）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/apollo_planning_flowchart.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里我只是简单介绍了 Apollo 中做 Planning 的过程，更多细节可以参见 Github 中已开源的代码，为了更好地了解也可以进一步阅读下面的参考资料。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;
1. &lt;a href=&#34;https://github.com/wolegechu/Apollo-Note&#34; target=&#34;_blank&#34;&gt;https://github.com/wolegechu/Apollo-Note&lt;/a&gt; (Forked from &lt;a href=&#34;https://github.com/YannZyl/Apollo-Note&#34; target=&#34;_blank&#34;&gt;YannZyl/Apollo-Note&lt;/a&gt;)
2. &lt;a href=&#34;https://github.com/ApolloAuto/apollo&#34; target=&#34;_blank&#34;&gt;ApolloAuto/apollo&lt;/a&gt;
3. &lt;a href=&#34;https://github.com/ApolloAuto/apollo/blob/master/modules/planning/README.md&#34; target=&#34;_blank&#34;&gt;Apollo 3.0 技术指南 - Planning 模块&lt;/a&gt;
4. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzI1NjkxOTMyNQ==&amp;amp;mid=2247485310&amp;amp;idx=1&amp;amp;sn=830f95c8fe1c0fe300ab4baa735361d5&amp;amp;chksm=ea1e150cdd699c1a149879f5d044b6ed866d0ae5d30c59aa571cdbcad2cad738c964c397fcdd&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0914fHtF0ADvSZWvZ7WSioeu&amp;amp;rd2werd=1#wechat_redirect&#34; target=&#34;_blank&#34;&gt;Apollo3.0 PnC更新以及车辆开放平台介绍&lt;/a&gt;
5. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzI1NjkxOTMyNQ==&amp;amp;mid=2247485354&amp;amp;idx=1&amp;amp;sn=c8f65591fe727904a8e1e440fb6827b9&amp;amp;chksm=ea1e15d8dd699cce46d296ede37177ccc8cc2f6c275ebd5c5b2f8008248f7d64de201e6ee1bf&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0914R7kJdKmCmX6XxA0NTUXp&amp;amp;rd2werd=1#wechat_redirect&#34; target=&#34;_blank&#34;&gt;Lattice Planner规划算法&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Deep Learning of Disentangled Representations to High-level Cognition</title>
      <link>/post/from_disentangled_to_high-level_congnition/</link>
      <pubDate>Wed, 21 Feb 2018 00:38:00 +0000</pubDate>
      
      <guid>/post/from_disentangled_to_high-level_congnition/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#still-far-from-human-level-ai&#34;&gt;Still Far from Human-level AI&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#paper-measuring-the-tendency-of-cnns-to-learn-surface-statistical-regularities&#34;&gt;Paper: Measuring the tendency of CNNs to Learn Surface Statistical Regularities&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learning-how-the-world-ticks&#34;&gt;Learning &amp;ldquo;How the world ticks&amp;rdquo;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#paper-learning-multiple-levels-of-abstraction&#34;&gt;Paper: Learning Multiple Levels of Abstraction&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#invariance-and-disentangling&#34;&gt;Invariance and Disentangling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-variables-and-abstract-representations&#34;&gt;Latent Variables and abstract Representations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paper-manifold-flattening&#34;&gt;Paper: Manifold Flattening&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-s-missing-and-what-s-needed-with-deep-learning-deep-understanding-and-abstract-representations&#34;&gt;What&amp;rsquo;s Missing and What&amp;rsquo;s Needed with Deep Learning? - Deep Understanding and Abstract Representations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#current-ml-theory-beyond-i-i-d-data&#34;&gt;Current ML theory Beyond i.i.d. Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-discover-good-disentangled-representations&#34;&gt;How to Discover Good Disentangled Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acting-to-guide-representation-learning-disentangling&#34;&gt;Acting to Guide Representation Learning &amp;amp; Disentangling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paper-independently-controllable-factors&#34;&gt;Paper: Independently Controllable Factors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#abstraction-challenge-for-unsupervised-learning&#34;&gt;Abstraction Challenge for Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paper-the-consciousness-prior-inspired-by-cognitive-psychology&#34;&gt;Paper: The Consciousness Prior (inspired by cognitive psychology)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#on-the-relation-between-abstraction-and-attention&#34;&gt;On the relation between Abstraction and Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-training-objective&#34;&gt;What Training Objective?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#consiciousness-prior-and-classical-ai&#34;&gt;Consiciousness Prior and Classical AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ongoing-research&#34;&gt;Ongoing Research&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#deep-unsupervised-rl-for-ai-neural-nets-cognition&#34;&gt;Deep Unsupervised RL for AI neural nets -&amp;gt; cognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ai-for-good-take-action&#34;&gt;AI for Good: take action!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;still-far-from-human-level-ai&#34;&gt;Still Far from Human-level AI&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Industrial successes mostly based on supervised learning&lt;/li&gt;
&lt;li&gt;Learning superficial clues, not generalizing well enough outside of training contexts, easy to fool trained networks:

&lt;ul&gt;
&lt;li&gt;Current models cheat by picking on surface regularities&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;paper-measuring-the-tendency-of-cnns-to-learn-surface-statistical-regularities&#34;&gt;Paper: Measuring the tendency of CNNs to Learn Surface Statistical Regularities&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hypothesis&lt;/strong&gt;: &lt;em&gt;Deep CNNs have a tendency to learn superficial statistical regularities in the dataset rather than high level abstract concepts.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;From the perspective of learning high level abstractions, Fourier image statistics can be &lt;em&gt;superficial&lt;/em&gt; regularities, not changing object category.&lt;/li&gt;
&lt;li&gt;Different Fourier filters, same high level abstractions (objects) but different surface statistical regularities (Fourier image statistics).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Experiments&lt;/strong&gt;: Train on one training set and evaluate the test sets.

&lt;ul&gt;
&lt;li&gt;A generalization gap: max difference in test accuracies&lt;/li&gt;
&lt;li&gt;Large generalization gap: CNN exploits too much of low level regularities, as opposed to learning the abstract high level concepts.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Intuition: we have a mental model that captures the explanatory factors of our world to some extent. It&amp;rsquo;s not perfect. And we can generalize to new configurations of the existing factor. When in new situation, involve concepts that we already know, thaen just combine  in very new ways.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;learning-how-the-world-ticks&#34;&gt;Learning &amp;ldquo;How the world ticks&amp;rdquo;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;So long as our machine learning models &amp;lt;&amp;lt; cheat &amp;gt;&amp;gt; by relying only on superficial statistical regularities, they remain vulnerable to out-of-distribution examples&lt;/li&gt;
&lt;li&gt;Humans generalize better than other animals thanks to a more accurate internal model of the &lt;strong&gt;underlying causal relationships&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;To predict future situations (e.g., the effect of planned actions) far from anything seen before while involving known concepts, an essential component of reasoning, intelligence and science&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;paper-learning-multiple-levels-of-abstraction&#34;&gt;Paper: Learning Multiple Levels of Abstraction&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The big payoff of deep learning is to allow learning higher levels of abstraction&lt;/li&gt;
&lt;li&gt;High-level abstractions &lt;strong&gt;disentangle the factors of variation&lt;/strong&gt;, which allows much easier generalization and transfer&lt;/li&gt;
&lt;li&gt;with Data -&amp;gt; Information -&amp;gt; Knowledge -&amp;gt; Wisdom

&lt;ul&gt;
&lt;li&gt;Organizational Maturity gain&lt;/li&gt;
&lt;li&gt;Abstraction Level gain&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;invariance-and-disentangling&#34;&gt;Invariance and Disentangling&lt;/h4&gt;

&lt;p&gt;The notion of disentangling is related but different from the notion of invariance, as being a very important notion in computer visions, speech recognition and so on, where we&amp;rsquo;d like to build detectors and features that are invariant to the things we don&amp;rsquo;t care about, but sensitive to the things we do care about.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Invariant features&lt;/li&gt;
&lt;li&gt;Alternative: Learning to disentangle factors&lt;/li&gt;
&lt;li&gt;Good disentangling -&amp;gt; avoid the curse of dimensionality&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;latent-variables-and-abstract-representations&#34;&gt;Latent Variables and abstract Representations&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180219-231015.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Encoder/decoder view: maps between low &amp;amp; high-levels&lt;/li&gt;
&lt;li&gt;Encoder doses inference: interpret the data at the abstract level&lt;/li&gt;
&lt;li&gt;Decoder can generate new configurations&lt;/li&gt;
&lt;li&gt;Encoder flattens and disentangles the data manifold&lt;/li&gt;
&lt;li&gt;Marginal independence in h-space (Can assemble from each of the factors if dimension is independently)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;paper-manifold-flattening&#34;&gt;Paper: Manifold Flattening&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Deeper representations -&amp;gt; abstractions -&amp;gt; disentangling&lt;/li&gt;
&lt;li&gt;Manifolds are expanded and flattened&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-s-missing-and-what-s-needed-with-deep-learning-deep-understanding-and-abstract-representations&#34;&gt;What&amp;rsquo;s Missing and What&amp;rsquo;s Needed with Deep Learning? - Deep Understanding and Abstract Representations&lt;/h2&gt;

&lt;h3 id=&#34;current-ml-theory-beyond-i-i-d-data&#34;&gt;Current ML theory Beyond i.i.d. Data&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Real-life applications often require generalizations in regimes not seen during training&lt;/li&gt;
&lt;li&gt;Humans can project themselves in situations they have never been (e.g. imagine being on another planet, or going through exceptional events like in many movies)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key: understanging explanatory/causal factors and mechanisms&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-to-discover-good-disentangled-representations&#34;&gt;How to Discover Good Disentangled Representations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;How to discover abstractions?&lt;/li&gt;
&lt;li&gt;What is a good representation? &lt;em&gt;(Bengio et al 2013)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Need clues (= priors) to help &lt;em&gt;disentangle&lt;/em&gt; the underlying factors, such as

&lt;ul&gt;
&lt;li&gt;Spatial &amp;amp; temporal scales&lt;/li&gt;
&lt;li&gt;Marginal independence&lt;/li&gt;
&lt;li&gt;Simple dependencies between factors

&lt;ul&gt;
&lt;li&gt;Consciousness prior&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Causal / mechanism independence

&lt;ul&gt;
&lt;li&gt;Controllable factors&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;acting-to-guide-representation-learning-disentangling&#34;&gt;Acting to Guide Representation Learning &amp;amp; Disentangling&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Some factors (e.g. objects) correspond to &amp;lsquo;independently controllable&amp;rsquo; aspects of the world&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Can only be discovered by acting in the world&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Control linked to notion of objects &amp;amp; agents&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Causal but agent-specific &amp;amp; subjective&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Acting to acquire information)&lt;/p&gt;

&lt;h3 id=&#34;paper-independently-controllable-factors&#34;&gt;Paper: Independently Controllable Factors&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Jointly train for each aspect (factor)

&lt;ul&gt;
&lt;li&gt;A policy $\pi_k$ (which tries to selectively change just that factor)

&lt;ul&gt;
&lt;li&gt;Kth policy is going to control the Kth factor&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A representation (which maps state to value of factor) $f_k$&lt;/li&gt;
&lt;li&gt;Discrete case, $\phi \in {1,..,N}$, define &lt;em&gt;selectivity&lt;/em&gt;:
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$ \sum_{k=1}^N{\mathbb{E}_{(s_t, a_t, s_{t+1})} [\pi_k{a_t|s_t} \cfrac{f_k(s_{t+1}) - f_k(s_t)}{\sum_{k&amp;rsquo;}{|f_{k&amp;rsquo;}(s_{t+1})f_{k&amp;rsquo;}(s_t)|}}]} $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimize both policy $\pi_k$ and representation $f_k$ to minimize

&lt;ul&gt;
&lt;li&gt;$$\mathbb{E}_s[\frac{1}{2} \lVert s - g(f(s)) \rVert^2_2] - \lambda \sum_k{\mathbb{E}_s[ \sum_a{\pi_k(a|s)\log{sel(s,a,k)}}]}$$&lt;/li&gt;
&lt;li&gt;Namely, (reconstruction error) - $\lambda$ (disentanglement objective)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Example 1.Predict the effect of actions in attribute space&lt;/li&gt;
&lt;li&gt;Example 2.Given two states, recover the casual actions leading from one to the other
Given initial state and set of actions, predict new attribute values and the corresponding reconstructed images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180220-175520.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Multi-step policies $\epsilon$ Embedding Space for Naming Factors and Policies
&lt;img src=&#34;/img/post/QQ20180220-180229.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;abstraction-challenge-for-unsupervised-learning&#34;&gt;Abstraction Challenge for Unsupervised Learning&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Why is modeling P(acoustics) so much worse than modeling P(acoustics|phonemes) P(phonemes)?&lt;/li&gt;
&lt;li&gt;Wrong level of abstraction?

&lt;ul&gt;
&lt;li&gt;many more entropy bits in acoustic details then linguistic content -&amp;gt; predict the future in abstract space instead&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;paper-the-consciousness-prior-inspired-by-cognitive-psychology&#34;&gt;Paper: The Consciousness Prior (inspired by cognitive psychology)&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Conscious thoughts are very low-dimensional objects compared to the full state of the (unconscious) brain&lt;/li&gt;
&lt;li&gt;Yet they have unexpected predictive value of usefulness -&amp;gt; strong constraint or prior on the underlying representation&lt;/li&gt;
&lt;li&gt;Other

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Thought&lt;/strong&gt;: composition of few selected factors / concepts (key/value) at the highest level of abstraction of our brain&lt;/li&gt;
&lt;li&gt;Richer than but closely associated with short verbal expression such as a &lt;strong&gt;sentence&lt;/strong&gt; or phrase, a &lt;strong&gt;rule&lt;/strong&gt; or &lt;strong&gt;fact&lt;/strong&gt; (link to classical symbolic AI &amp;amp; knowledge representation)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How to select a few relevant abstract concepts making a thought?&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Content-based  Attention&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;on-the-relation-between-abstraction-and-attention&#34;&gt;On the relation between Abstraction and Attention&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Attention allows to focus on a few elements out of a large set&lt;/li&gt;
&lt;li&gt;Soft-attention allows this process to be trainable with gradient-based optimization and backprop&lt;/li&gt;
&lt;li&gt;Attention focuses on a few appropriate abstract or concret elements of mental representation&lt;/li&gt;
&lt;li&gt;Access consciousness is one aspect of consciousness, what is accessed and comes to mind (Dehaene&amp;rsquo;s C1 or global availability), See &lt;em&gt;Dehaene et al, Science, 2017.&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;/img/post/QQ20180221-044638.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2 levels of representation

&lt;ul&gt;
&lt;li&gt;High-dimensional abstract representation space (all known concepts and factors) &lt;em&gt;h&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Low-dimensional conscious thought &lt;em&gt;c&lt;/em&gt;, extracted from &lt;em&gt;h&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;/img/post/QQ20180221-045704.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;c&lt;/em&gt; includes names (keys) and values of factors&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Conscious prediction over attended variables A (soft-attention)

&lt;ul&gt;
&lt;li&gt;$$V = - \sum_A{w_a \log{P(h_{t,A} = a | c_{t-1})}}$$

&lt;ul&gt;
&lt;li&gt;$w_A$: Attention weights&lt;/li&gt;
&lt;li&gt;$h_{t,A}$: Factor &lt;strong&gt;name&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$a$: Predicted &lt;strong&gt;value&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;$c_{t-1}$: Earlier conscious state
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;what-training-objective&#34;&gt;What Training Objective?&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;How to train the attention mechanism which selects which variables to predict?

&lt;ul&gt;
&lt;li&gt;Representation learning without reconstruction:

&lt;ul&gt;
&lt;li&gt;Maximize entropy of code (preserve a lot of information about the data)&lt;/li&gt;
&lt;li&gt;Maximize mutual information between past and future representations (see also &lt;em&gt;Becker &amp;amp; Hinton 1992&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Objective function completely in abstract space, higher-level parameters model dependencies in abstract space&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Usefulness of thoughts: as conditioning information for action, i.e., a particular form of planning for RL, i.e., the estimated gradient of rewards could also be used to drive learning of abstract representations&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;consiciousness-prior-and-classical-ai&#34;&gt;Consiciousness Prior and Classical AI&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Conscious thought is closedly related to a linguistic utterance&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;(conditioning variables -&amp;gt; predicted variables) = a &lt;strong&gt;rule&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Conscious thoughts &amp;amp; short-term memory: good level of representation for memorization and memories&lt;/li&gt;
&lt;li&gt;Need to refer to variables in conscious states facilitates recursive compositional computation&lt;/li&gt;
&lt;li&gt;Consciousness makes it easier to associate preception, action and &lt;strong&gt;natural language&lt;/strong&gt; and use &lt;em&gt;natural language data to guide the creation of high-level abstraction expressed linguistically&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ongoing-research&#34;&gt;Ongoing Research&lt;/h2&gt;

&lt;h3 id=&#34;deep-unsupervised-rl-for-ai-neural-nets-cognition&#34;&gt;Deep Unsupervised RL for AI neural nets -&amp;gt; cognition&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Learn more abstract representations which capture causality&lt;/li&gt;
&lt;li&gt;Disentangle controllable factors&lt;/li&gt;
&lt;li&gt;Naturally gives rise to the notion of &lt;strong&gt;objects, attributes, agents &amp;amp; goals&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Natural language &amp;amp; consciousness prior: other clue about abstract representations (see also &lt;em&gt;Andreas, Klein &amp;amp; Levine 2017&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Unsupervised RL research, performed in gradually more challenging sumulated environments&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ai-for-good-take-action&#34;&gt;AI for Good: take action!&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Beyond developing the next gadget&lt;/li&gt;
&lt;li&gt;Actionalbe items:

&lt;ul&gt;
&lt;li&gt;Favor ML applications which help the poorest countries, may help with fighting climate change, etc.&lt;/li&gt;
&lt;li&gt;A role for the Partnership on AI: fund an organization which will coordinate, prioritize and channel funding for such applications, as well as facilitate internships for students from poor conuntries in top ML labs&lt;/li&gt;
&lt;li&gt;Accept such interns even if they are below our bar, they will return home with knowledge of the state-of-the-art in ML&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[OS Notes] Processes</title>
      <link>/post/processes/</link>
      <pubDate>Mon, 01 Jan 2018 00:38:00 +0000</pubDate>
      
      <guid>/post/processes/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#unic-centric-user-view-of-processes&#34;&gt;(UNIC-centric) User view of processes&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating-processes&#34;&gt;Creating processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deleting-processes&#34;&gt;Deleting processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#running-programs&#34;&gt;Running programs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#manipulating-file-descriptors&#34;&gt;Manipulating file descriptors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pipes&#34;&gt;Pipes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-fork&#34;&gt;Why fork?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kernel-view-of-processes&#34;&gt;Kernel view of processes&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#implementing-processes&#34;&gt;Implementing processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#process-states&#34;&gt;Process states&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scheduling&#34;&gt;Scheduling&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#policy&#34;&gt;Policy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preemption&#34;&gt;Preemption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#context-switch&#34;&gt;Context switch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#threads&#34;&gt;Threads&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#threads-package-api&#34;&gt;Threads package API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kernel-threads&#34;&gt;Kernel threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitation-of-kernel-level-threads&#34;&gt;Limitation of kernel-level threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#alternative-user-threads&#34;&gt;Alternative: User threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementing-user-level-threads&#34;&gt;Implementing user-level threads&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thread-implementation-details&#34;&gt;Thread implementation details&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backgroud&#34;&gt;Backgroud&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#calling-conventions&#34;&gt;Calling conventions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#procedure-calls&#34;&gt;Procedure calls&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pintos-thread-implementation&#34;&gt;Pintos thread implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#i386-switch-threads&#34;&gt;i386 switch_threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitations-of-user-level-threads&#34;&gt;Limitations of user-level threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#user-threads-on-kernel-threads&#34;&gt;User threads on kernel threads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitation-of-n-m-threading&#34;&gt;Limitation of n:m threading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lessons&#34;&gt;Lessons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;unic-centric-user-view-of-processes&#34;&gt;(UNIC-centric) User view of processes&lt;/h2&gt;

&lt;h3 id=&#34;creating-processes&#34;&gt;Creating processes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Original UNIX paper&lt;/strong&gt; is a great reference on core system calls

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;int fork (void);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Create new process that is exact copy of current one&lt;/li&gt;
&lt;li&gt;Returen &lt;em&gt;process ID&lt;/em&gt; of new process in &amp;ldquo;parent&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Return 0 in &amp;ldquo;child&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;int waitpid (int pid, int *stat, int opt);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;pid - process to wait for, or -1 for any&lt;/li&gt;
&lt;li&gt;stat - will contain exit value, or signal&lt;/li&gt;
&lt;li&gt;opt - usually 0 or WNOHANG&lt;/li&gt;
&lt;li&gt;Returns process ID or -1 on error&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;deleting-processes&#34;&gt;Deleting processes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;void exit (int status);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Current process ceases to exist&lt;/li&gt;
&lt;li&gt;&lt;em&gt;status&lt;/em&gt; shows up in &lt;em&gt;waitpid&lt;/em&gt; (shifted)&lt;/li&gt;
&lt;li&gt;By convention, status of 0 is success, non-zero error&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;int kill (int pid, int sig)&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Sends signal &lt;em&gt;sig&lt;/em&gt; to process &lt;em&gt;pid&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;SIGTERM&lt;/em&gt; most common value, kills process by default (but application can catch it for &amp;ldquo;cleanup&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;SIGKILL&lt;/em&gt; stronger, kills process always&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;running-programs&#34;&gt;Running programs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;int execve (char *prog, char **argv, char **envp);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;prog - full pathname of program to run&lt;/li&gt;
&lt;li&gt;argv - argument vector that gets passed to &lt;em&gt;main&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;envp - environment variables, e.g., PATH, HOME&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Generally called through a wrapper functions

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;int execvp (char *prog, char **argv);&lt;/code&gt; Search PATH for prog, use current environment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;int execlp (char *prog, char *arg, ...);&lt;/code&gt; List arguments one at a time, finish with &lt;em&gt;NULL&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Warning: Pintos exec more like combined fork/exec&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Simplified minish.c&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;pid_t pid; char **av;
void do exec(){
    execvp (av[0], av);
    perror (av[0]);
    exit (1);
}
/* ... main loop: */
for (;;) {
    parse_next_line_of_input (&amp;amp;av, stdin)
    switch (pid = fork()) {
        case -1:
            perror (&amp;quot;fork&amp;quot;); break;
        case 0:
            doexec();
        default:
            waitpid (pid, NULL, o); break;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;manipulating-file-descriptors&#34;&gt;Manipulating file descriptors&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;int dup2 (int oldfd, int newfd);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Closes &lt;em&gt;newfd&lt;/em&gt;, if it was a valid descriptor&lt;/li&gt;
&lt;li&gt;Makes &lt;em&gt;newfd&lt;/em&gt; an exact copy of &lt;em&gt;oldfd&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Two file descriptors will share some offset (lseek on one will affect both)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;int fcntl (int fd, F_SETFD, int val)&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Sets &lt;em&gt;close on exec&lt;/em&gt; flag if val = 1, clears if val = 0&lt;/li&gt;
&lt;li&gt;Make file descriptor non-inheritable by spawned programs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Example: redirsh.c

&lt;ul&gt;
&lt;li&gt;Loop that reads a command and executes it&lt;/li&gt;
&lt;li&gt;Recognizes command &amp;lt; input &amp;gt; output 2&amp;gt; errlog&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;void doexec (void) {
    int fd;
    if (infile) {  /* non-NULL for &amp;quot;command &amp;lt; infile&amp;quot; */
        if ((fd = open (infile, O_RDONLY)) &amp;lt; 0){
            perror (infile);
            exit (1);
        }
        if (fd != 0) {
            dup2 (fd, 0);
            close (fd);
        }
    }

    /* ... do same for outfile-&amp;gt;fd 1, errfile-&amp;gt;fd 2 ... */
    execvp (av[0], av);
    perror (av[0]);
    exit(1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pipes&#34;&gt;Pipes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;int pipe (int fds[2]);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Returns two descriptors in &lt;em&gt;fds[0]&lt;/em&gt; and &lt;em&gt;fds[1]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Data written to &lt;em&gt;fds[1]&lt;/em&gt; will be returned by &lt;em&gt;read&lt;/em&gt; on &lt;em&gt;fds[0]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;When last copy of fds[1] closed, fds[0] will return &lt;em&gt;EOF&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Returns 0 on success, -1 on error&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Operations on pipes

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;read/write/close&lt;/em&gt; - as with files&lt;/li&gt;
&lt;li&gt;When &lt;em&gt;fds[1]&lt;/em&gt; closed, read(fds[0]) returns 0 bytes&lt;/li&gt;
&lt;li&gt;When &lt;em&gt;fds[0]&lt;/em&gt; closed, write(fds[1]):

&lt;ul&gt;
&lt;li&gt;Kills process with SIGPIPE&lt;/li&gt;
&lt;li&gt;Or if signal ignored, fails with EPIPE&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Example: pipesh.c

&lt;ul&gt;
&lt;li&gt;Stes up pipeline command1 | command2 | command3 &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;void doexec (void) {
    while (outcmd) {
        int pipefds[2]; pipe (pipefds);
        switch (fork()){
            case -1:
                perror (&amp;quot;fork&amp;quot;); exit (1);
            case 0:
                dup2 (pipefds[1], 1);
                close (pipefds[0]); close (pipefds[1]);
                outcmd = NULL;
                break;
            default:
                dup2 (pipefds[0], 0);
                close (pipefds[0]); close (pipdefds[1]);
                parse_command_line (&amp;amp;av, &amp;amp;outcomd, outcomd);
                break;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-fork&#34;&gt;Why fork?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Most calls to &lt;em&gt;fork&lt;/em&gt; followed by &lt;em&gt;execve&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Could also combine into one &lt;em&gt;spawn&lt;/em&gt; system call (like Pintos exec)&lt;/li&gt;
&lt;li&gt;Occasionally useful to fork one process

&lt;ul&gt;
&lt;li&gt;Unix &lt;em&gt;dump&lt;/em&gt; utility backs up file system to tape&lt;/li&gt;
&lt;li&gt;If tape fills up, must restart at some logical point&lt;/li&gt;
&lt;li&gt;Implemented by forking to revert to old state if tape ends&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Real win is simplicity of interface

&lt;ul&gt;
&lt;li&gt;Tons of things you might want to do to child: Manipulate file descriptors, set encironment variables, reduce privileges, &amp;hellip;&lt;/li&gt;
&lt;li&gt;Yet &lt;em&gt;fork&lt;/em&gt; requires no arguments at all&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Spawning a process without fork

&lt;ul&gt;
&lt;li&gt;Without fork, needs tons of different options for new process&lt;/li&gt;
&lt;li&gt;Example: Windows &lt;em&gt;CreateProcess&lt;/em&gt; System call&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kernel-view-of-processes&#34;&gt;Kernel view of processes&lt;/h2&gt;

&lt;h3 id=&#34;implementing-processes&#34;&gt;Implementing processes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Keep a data structure for each process

&lt;ul&gt;
&lt;li&gt;Process Control Block (PCB)&lt;/li&gt;
&lt;li&gt;Called &lt;em&gt;proc&lt;/em&gt; in Unix, &lt;em&gt;task_struct&lt;/em&gt; in Linux, and just &lt;em&gt;struct thread&lt;/em&gt; in Pintos&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tracks &lt;em&gt;state&lt;/em&gt; of the process

&lt;ul&gt;
&lt;li&gt;Running, ready (runnable), waiting, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Includes information necessary to run

&lt;ul&gt;
&lt;li&gt;Registers, virtual memory mappings, etc.&lt;/li&gt;
&lt;li&gt;Open files (including memory mapped files)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Various other data about the process

&lt;ul&gt;
&lt;li&gt;Credentials (user/group ID), signal mask, controlling terminal, priority, accounting statistics, whether being debugged, which system call binary emulation in use,..&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;process-states&#34;&gt;Process states&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180102-100721.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Process can be in one of several states

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;new&lt;/em&gt; &amp;amp; &lt;em&gt;terminated&lt;/em&gt; at beginning &amp;amp; end of life&lt;/li&gt;
&lt;li&gt;&lt;em&gt;running&lt;/em&gt; - currently executing (or will execute on kernel return)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ready&lt;/em&gt; - can run, but kernel has chosen different process to run&lt;/li&gt;
&lt;li&gt;&lt;em&gt;waiting&lt;/em&gt; - needs async event (e.g., disk operation)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Which process should kernel run?

&lt;ul&gt;
&lt;li&gt;if 0 runnable, run idle loop (or halt CPU), if 1 runnable, run it&lt;/li&gt;
&lt;li&gt;if &amp;gt; 1 runnable, must make scheduling decision&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;scheduling&#34;&gt;Scheduling&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;How to pick which process to run&lt;/li&gt;
&lt;li&gt;Scan process table for first runnable&lt;/li&gt;
&lt;li&gt;FIFO?&lt;/li&gt;
&lt;li&gt;Priority?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;policy&#34;&gt;Policy&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Want to balance multiple goals

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Fairness&lt;/em&gt; - don&amp;rsquo;t starve processes&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Priority&lt;/em&gt; - reflect relative importance of procs&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Deadlines&lt;/em&gt; - must do X (play audio) by certain time&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Throughput&lt;/em&gt; - want good overall performance&lt;/li&gt;
&lt;li&gt;&lt;em&gt;EfficiencyJ&lt;/em&gt; - minimize overhead of scheduler itself&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;No universal policy

&lt;ul&gt;
&lt;li&gt;Many variables, can&amp;rsquo;t optimize for all&lt;/li&gt;
&lt;li&gt;Conflicting goals (e.g., throughput or priority vs. fairness)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;preemption&#34;&gt;Preemption&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Can preempt a process when kernel gets control&lt;/li&gt;
&lt;li&gt;Running process can vector control to kernel

&lt;ul&gt;
&lt;li&gt;System call, page fault, illegal instruction, etc.&lt;/li&gt;
&lt;li&gt;May put current process to sleep&lt;/li&gt;
&lt;li&gt;May make other process runnable&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Periodic timer interrupt

&lt;ul&gt;
&lt;li&gt;If running process used up quantum, schedule another&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Device interrupt

&lt;ul&gt;
&lt;li&gt;Disk request completed, or packet arrived on network&lt;/li&gt;
&lt;li&gt;Previously waiting process becomes runnable&lt;/li&gt;
&lt;li&gt;Schedule if higher priority than current running proc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Changing running process is called a &lt;em&gt;context switch&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;context-switch&#34;&gt;Context switch&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180102-102427.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Very machine dependent. Typical things include:

&lt;ul&gt;
&lt;li&gt;Save program counter and integer registers (always)&lt;/li&gt;
&lt;li&gt;Save floating point or other special registers&lt;/li&gt;
&lt;li&gt;Save condition codes&lt;/li&gt;
&lt;li&gt;Change virtual address translations&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Non-negligible cost

&lt;ul&gt;
&lt;li&gt;Save/restore floating point registers expensive

&lt;ul&gt;
&lt;li&gt;Optimization: only save if process used floating point&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;May require flushing TLB (memory translation hardware)&lt;/li&gt;
&lt;li&gt;Usually causes more cache misses (switch working sets)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;threads&#34;&gt;Threads&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180102-102842.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Most popular abstraction for concurrency

&lt;ul&gt;
&lt;li&gt;Lighter-weight abstraction than processes&lt;/li&gt;
&lt;li&gt;All threads in one process share memory, filed escriptors, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Allows one process to use multiple CPUs or cores&lt;/li&gt;
&lt;li&gt;Allows program to overlap I/O and computation

&lt;ul&gt;
&lt;li&gt;Same benefit as OS running emacs &amp;amp; gcc simultaneously&lt;/li&gt;
&lt;li&gt;E.g., threaded web server services clients simultaneously:&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;for (;;) {
    fd = accept_client ();
    thread_create (service_client, &amp;amp;fd);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Most kernels have threads, too&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;threads-package-api&#34;&gt;Threads package API&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tid thread_create (void (*fn) (void *), void *);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Create a new thread, run &lt;em&gt;fn&lt;/em&gt; with &lt;em&gt;arg&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;void thread_exit ();&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Destroy current thread&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;void thread_join (tid thread);&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Wait for thread &lt;em&gt;thread&lt;/em&gt; to exit&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Can have preemptive or non-preemptive threads

&lt;ul&gt;
&lt;li&gt;Preemptive causes more race conditions&lt;/li&gt;
&lt;li&gt;Non-preemptive can&amp;rsquo;t take advantage of multiple CPUs&lt;/li&gt;
&lt;li&gt;Before prevalent SMPs, most kernels non-preemptive&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;kernel-threads&#34;&gt;Kernel threads&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180102-104119.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Can implement &lt;em&gt;thread_create&lt;/em&gt; as a system call&lt;/li&gt;
&lt;li&gt;To add &lt;em&gt;thread_create&lt;/em&gt; to an OS that doesn&amp;rsquo;t have it:

&lt;ul&gt;
&lt;li&gt;Start with process abstraction in kernel&lt;/li&gt;
&lt;li&gt;&lt;em&gt;thread_create&lt;/em&gt; like process creation with features stripped out

&lt;ul&gt;
&lt;li&gt;Keep same address space, file table, etc., in new process&lt;/li&gt;
&lt;li&gt;&lt;em&gt;rfork/clone&lt;/em&gt; syscalls actually allow individual control&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Faster than a process, but still very heacy weight&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;limitation-of-kernel-level-threads&#34;&gt;Limitation of kernel-level threads&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Every thread operation must go through kernel

&lt;ul&gt;
&lt;li&gt;create, exit, join, synchroniza, or switch for any reason&lt;/li&gt;
&lt;li&gt;On my laptop: syscall takes 100 cycles, fn call 5 cycles&lt;/li&gt;
&lt;li&gt;Result: threads 10x-30x slower when implemented in kernel&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One-size fits all thread implementation

&lt;ul&gt;
&lt;li&gt;Kernel threads must please all people&lt;/li&gt;
&lt;li&gt;Maybe pay for fancy features (priority, etc.) you don&amp;rsquo;t need&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;General heavy-weight memory requirements

&lt;ul&gt;
&lt;li&gt;E.g., requires a fixed-size stack whthin kernel&lt;/li&gt;
&lt;li&gt;Other data structures desighned for heavier-weight processes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;alternative-user-threads&#34;&gt;Alternative: User threads&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180102-104743.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implement as user-level library (a.k.a. &lt;em&gt;green&lt;/em&gt; threads)

&lt;ul&gt;
&lt;li&gt;One kernel thread per process&lt;/li&gt;
&lt;li&gt;&lt;em&gt;thread_create&lt;/em&gt;, &lt;em&gt;thread_exit&lt;/em&gt;, etc., just library functions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;implementing-user-level-threads&#34;&gt;Implementing user-level threads&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Allocate a new stack for each &lt;em&gt;thread_create&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Keep a queue of runnable threads&lt;/li&gt;
&lt;li&gt;Replace networking system calls (read/write/etc.)

&lt;ul&gt;
&lt;li&gt;If operation would block, switch and run different thread&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Schedule periodic timer signal (setitimer)

&lt;ul&gt;
&lt;li&gt;Switch to another thread on timer signals&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Muti-threaded web server example

&lt;ul&gt;
&lt;li&gt;Thread calls &lt;em&gt;read&lt;/em&gt; to get data from remote web browser&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Fake&amp;rdquo; read &lt;em&gt;function&lt;/em&gt; makes read &lt;em&gt;syscall&lt;/em&gt; in non-blocking mode&lt;/li&gt;
&lt;li&gt;No data? schedule another thread&lt;/li&gt;
&lt;li&gt;On timer or when idle check which connections have new data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;thread-implementation-details&#34;&gt;Thread implementation details&lt;/h2&gt;

&lt;h3 id=&#34;backgroud&#34;&gt;Backgroud&lt;/h3&gt;

&lt;h4 id=&#34;calling-conventions&#34;&gt;Calling conventions&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/1111.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Registers divided into 2 groups

&lt;ul&gt;
&lt;li&gt;Functions free to clobber &lt;strong&gt;caller-saved&lt;/strong&gt; regs (%eax [return val], %edx, &amp;amp;%ecx on x86)&lt;/li&gt;
&lt;li&gt;But must restore &lt;strong&gt;callee-saved&lt;/strong&gt; ones to original value upon return (on x86, %ebx, %esi, %edi, plus %ebp and %esp)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;sp&lt;/em&gt; register always base of stack

&lt;ul&gt;
&lt;li&gt;Frame pointer (fp) is old &lt;em&gt;sp&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Local variables stored in registers and on stack&lt;/li&gt;
&lt;li&gt;Function arguments go in caller-saved regs and on stack

&lt;ul&gt;
&lt;li&gt;with 32-bit x86, all arguments on stack&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;procedure-calls&#34;&gt;Procedure calls&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Procedure call&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;save active caller registers
call foo (pushes pc) -&amp;gt;
      save used callee registers
      &amp;hellip;do stuff&amp;hellip;
      restore callee saved registers
      jump back to calling function
      &amp;lt;-
restore caller registers&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Caller must save some state across function call

&lt;ul&gt;
&lt;li&gt;Return address, caller-saved registers&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Other state does not need to be saved

&lt;ul&gt;
&lt;li&gt;Calle-saved regs, global variables, stack pointer&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;pintos-thread-implementation&#34;&gt;Pintos thread implementation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Implemente user processes on top of its own threads

&lt;ul&gt;
&lt;li&gt;Same technique can be used to implement user-level threads, too&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Per-thread state in thread control block structure&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;struct thread {
...
uint8_t *stack; /* Saved stack pointer. */
...
};
uint32_t thread_stack_ofs = offsetof(struct thread, stack)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;C declaration for asm thread-switch function&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;struct thread *switch_threads (struct thread *cur, struct thread *next);
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Also thread initialization function to create new stack:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void thread_create (const char *name, thread_func *function, void *aux);
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;i386-switch-threads&#34;&gt;i386 switch_threads&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;pushl %ebx; pushl %ebp # Save callee-saved regs
pushl %esi; pushl %edi
mov thread_stack_ofs, %edx # %edx = offset of stack field
# in thread struct
movl 20(%esp), %eax # %eax = cur
movl %esp, (%eax,%edx,1) # cur-&amp;gt;stack = %esp
movl 24(%esp), %ecx # %ecx = next
movl (%ecx,%edx,1), %esp # %esp = next-&amp;gt;stack
popl %edi; popl %esi # Restore calle-saved regs
popl %ebp; popl %ebx
ret # Resume execution
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;limitations-of-user-level-threads&#34;&gt;Limitations of user-level threads&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;A user-level thread library can do the same thing as Pintos&lt;/li&gt;
&lt;li&gt;Can&amp;rsquo;t take advantage of multiple CPUs or cores&lt;/li&gt;
&lt;li&gt;A blocking system call blocks all threads

&lt;ul&gt;
&lt;li&gt;Can replace &lt;em&gt;read&lt;/em&gt; to handle network connections&lt;/li&gt;
&lt;li&gt;But usually OSes don&amp;rsquo;t let you do this for disk&lt;/li&gt;
&lt;li&gt;So one uncached disk read blocks all threads&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A page fault blocks all threads&lt;/li&gt;
&lt;li&gt;Possible deadlock if one thread blocks on another&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;user-threads-on-kernel-threads&#34;&gt;User threads on kernel threads&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;User threads implemented on kernel threads

&lt;ul&gt;
&lt;li&gt;Multiple kernel-level threads per process&lt;/li&gt;
&lt;li&gt;&lt;em&gt;thread_create, thread_exit&lt;/em&gt; still library functions as before&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Sometimes called &lt;em&gt;n:m&lt;/em&gt; threading

&lt;ul&gt;
&lt;li&gt;Have &lt;em&gt;n&lt;/em&gt; user threads per &lt;em&gt;m&lt;/em&gt; kernel threads (simple user-level threads are n:1, kernel threads 1:1)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;limitation-of-n-m-threading&#34;&gt;Limitation of n:m threading&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Many of same problems as n:1 threads

&lt;ul&gt;
&lt;li&gt;Blocked threads, deadlock&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Hard to keep same # kthreads as availabel CPUs

&lt;ul&gt;
&lt;li&gt;Kernel knows how many CPUs available&lt;/li&gt;
&lt;li&gt;Kernel knows which kernel-level threads are blocked&lt;/li&gt;
&lt;li&gt;But tries to hide these things from applications for transparency&lt;/li&gt;
&lt;li&gt;So user-level thread scheduler might think a thread is running while underlying kernel thread is blocked&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Kernel doesn&amp;rsquo;t know relative importance of threads

&lt;ul&gt;
&lt;li&gt;Might preempt kthread in which library holds important lock&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lessons&#34;&gt;Lessons&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Threads best implemented as a library

&lt;ul&gt;
&lt;li&gt;But kernel threads not best interface on which to do this&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Better kernel interface have been suggested

&lt;ul&gt;
&lt;li&gt;Scheduler Activations&lt;/li&gt;
&lt;li&gt;Maybe too complex to implement on exisiting OSes (some have added then removed such features, now Windows is trying it)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Standard threads still fine for most purposes

&lt;ul&gt;
&lt;li&gt;Use kernel threads if I/O concurrency main goal&lt;/li&gt;
&lt;li&gt;Use n:m threads for highly concurrent (e.g., sicentific applications) with many threads switches&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;But concurrency greatly increases complexity&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[OS Notes] Synchronization (Part I)</title>
      <link>/post/synchronization_1/</link>
      <pubDate>Mon, 01 Jan 2018 00:38:00 +0000</pubDate>
      
      <guid>/post/synchronization_1/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cache-coherence-the-hardware-view&#34;&gt;Cache coherence (the hardware view)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#multicore-caches&#34;&gt;Multicore Caches&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mesi-coherence-protocol&#34;&gt;MESI coherence protocol&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#core-and-bus-actions&#34;&gt;Core and Bus Actions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cc-numa&#34;&gt;cc-NUMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#real-world-coherence-costs&#34;&gt;Real World Coherence Costs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#numa-and-spinlocks&#34;&gt;NUMA and spinlocks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#synchronization-and-memory-consistency-review&#34;&gt;Synchronization and memory consistency review&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#amdahl-s-law&#34;&gt;Amdahl&amp;rsquo;s law&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#locking-basics&#34;&gt;Locking basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#locking-granularity&#34;&gt;Locking granularity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;cache-coherence-the-hardware-view&#34;&gt;Cache coherence (the hardware view)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Important memory system properties

&lt;ul&gt;
&lt;li&gt;Coherence - concerns accesses to a single memory location

&lt;ul&gt;
&lt;li&gt;Must obey program order if access from only CPU&lt;/li&gt;
&lt;li&gt;There is a total order on all updates&lt;/li&gt;
&lt;li&gt;There is bounded latency before everyone sees a write&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Consistency - concerns ordering across memory locations

&lt;ul&gt;
&lt;li&gt;Even with coherence, different CPUs can see the same write happen at different times&lt;/li&gt;
&lt;li&gt;Sequential consistency is what matches our intuition (As if instructions from all CPUs interleaved on one CPU)&lt;/li&gt;
&lt;li&gt;Many architectures offer weaker consistency can still be sufficient to implement thread API&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;multicore-caches&#34;&gt;Multicore Caches&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Performance requires caches

&lt;ul&gt;
&lt;li&gt;Divided into chuncks of bytes called lines (e.g., 64 bytes)&lt;/li&gt;
&lt;li&gt;Caches create an opportunity for cores to disagree about memory&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Bus-based approaches

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Snoopy&amp;rdquo; protocols, each CPU listens to memory bus&lt;/li&gt;
&lt;li&gt;Use write-through and invalidate when you see a write bits&lt;/li&gt;
&lt;li&gt;Bus-based schemes limit scalability&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Modern CPUs use networks (e.g., hypertransport, QPI)

&lt;ul&gt;
&lt;li&gt;CPUs pass each other messages about cache lines&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;mesi-coherence-protocol&#34;&gt;MESI coherence protocol&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Modified

&lt;ul&gt;
&lt;li&gt;One cache has a valid copy&lt;/li&gt;
&lt;li&gt;That copy is dirty (needs to be written back to memory)&lt;/li&gt;
&lt;li&gt;Must invalidate all copies in other caches before entering this state&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Exclusive

&lt;ul&gt;
&lt;li&gt;Same as Modified except the cache copy is clean&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Shared

&lt;ul&gt;
&lt;li&gt;One or more caches and memory have a valid copy&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Invalid

&lt;ul&gt;
&lt;li&gt;Doesn&amp;rsquo;t contain any data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Owned (for enhanced &amp;ldquo;MOESI&amp;rdquo; protocal)

&lt;ul&gt;
&lt;li&gt;Memory may contain stale value of data (like Modified state)&lt;/li&gt;
&lt;li&gt;But have to broadcast modifications (sort of like Shared state)&lt;/li&gt;
&lt;li&gt;Can have both one owned and mutiple shared copies of cache line&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;core-and-bus-actions&#34;&gt;Core and Bus Actions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Core

&lt;ul&gt;
&lt;li&gt;Read&lt;/li&gt;
&lt;li&gt;Write&lt;/li&gt;
&lt;li&gt;Evict (modified? must write back)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Bus

&lt;ul&gt;
&lt;li&gt;Read: without intent to modify, data can come from memory or another cache&lt;/li&gt;
&lt;li&gt;Read-exclusive: with intent to modify, must invalidate all other cache copies&lt;/li&gt;
&lt;li&gt;Writeback: contens put on bus and memory is updated&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;cc-numa&#34;&gt;cc-NUMA&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Old machines used &lt;em&gt;dance hall&lt;/em&gt; achitectures

&lt;ul&gt;
&lt;li&gt;Any CPU can &amp;ldquo;dance with&amp;rdquo; any memory equally&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;An alternative: Non-Uniform Memory Access

&lt;ul&gt;
&lt;li&gt;Each CPU has fast access to some &amp;ldquo;close&amp;rdquo; memory&lt;/li&gt;
&lt;li&gt;Slower to access memory that is farther away&lt;/li&gt;
&lt;li&gt;Use a directory to keep track of who is caching what&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Originally for esoteric machines with many CPUs

&lt;ul&gt;
&lt;li&gt;But AMD and then Intel integrated memory controller into CPU&lt;/li&gt;
&lt;li&gt;Faster to access memory controlled by the local socket (or even local die in a multi-chip module)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;cc-NUMA = cache-coherent non-uniform memory access

&lt;ul&gt;
&lt;li&gt;Rarely see non-cache-coherent NUMA&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;real-world-coherence-costs&#34;&gt;Real World Coherence Costs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;If another core in same socket holds line in modified state:

&lt;ul&gt;
&lt;li&gt;load: 109 cycles (LLC + 65)&lt;/li&gt;
&lt;li&gt;store: 115 cycles (LLC + 71)&lt;/li&gt;
&lt;li&gt;atomic CAS: 120 cycles (LLC + 76)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If a core in a different socket holds line in modiefied state

&lt;ul&gt;
&lt;li&gt;NUMA load: 289 cycles&lt;/li&gt;
&lt;li&gt;NUMA store: 320 cycles&lt;/li&gt;
&lt;li&gt;NUMA atomic CAS: 324 cycles&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;But only a partial picture

&lt;ul&gt;
&lt;li&gt;Could be faster because of out-of-order execution&lt;/li&gt;
&lt;li&gt;Could be slower if interconnect contention or multiple hops&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;numa-and-spinlocks&#34;&gt;NUMA and spinlocks&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Test-and-set spinlock has serveral advantages

&lt;ul&gt;
&lt;li&gt;Simple to implement and understand&lt;/li&gt;
&lt;li&gt;One memory location for arbitrarily many CPUs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;But also has disadvantages

&lt;ul&gt;
&lt;li&gt;Lots of traffic over memory bus (especially when &amp;gt; 1 spinner)&lt;/li&gt;
&lt;li&gt;Not neccessarily fair (same CPU acquires lock many times)&lt;/li&gt;
&lt;li&gt;Even less fair on a NUMA machine&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Idea 1: Avoid spinlocks altogether (today)&lt;/li&gt;
&lt;li&gt;Idea 2: Reduce bus traffic with better spinlocks (next lecture)

&lt;ul&gt;
&lt;li&gt;Design lock that spins only on local memory&lt;/li&gt;
&lt;li&gt;Also gives better fairness&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;synchronization-and-memory-consistency-review&#34;&gt;Synchronization and memory consistency review&lt;/h2&gt;

&lt;h3 id=&#34;amdahl-s-law&#34;&gt;Amdahl&amp;rsquo;s law&lt;/h3&gt;

&lt;p&gt;$$T(n)=T(1)(B+1/n(1-B))$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Expected speedup limited when only part of a task is sped up

&lt;ul&gt;
&lt;li&gt;$T(n)$: the time it takes n CPU cores to complete the task&lt;/li&gt;
&lt;li&gt;B: the fraction of the job that must be serial&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Even with massive multiprocessors, $\lim_{n\to \infty} = B \cdot T(1) $
&lt;img src=&#34;/img/post/QQ20180103-233445.png&#34; alt=&#34;&#34; /&gt;

&lt;ul&gt;
&lt;li&gt;Places an ultimate limit on parallel speedup&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Problem: synchronization increases serial section size&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;locking-basics&#34;&gt;Locking basics&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;mutex_t m;
lock(&amp;amp;m);
cnt = cnt + 1; /* critical section */
unlock(&amp;amp;m);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Only one thread can hold a mutex at a time

&lt;ul&gt;
&lt;li&gt;Makes critical section atomic&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Recall &lt;strong&gt;thread API contract&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;All access to global data must be protected by a mutex&lt;/li&gt;
&lt;li&gt;Global = two or more threads touch data and at least one writes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Means must map each piece of global data to one mutex

&lt;ul&gt;
&lt;li&gt;Never touch the data unless you locked that mutex&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;But many ways to map data to mutexes&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;locking-granularity&#34;&gt;Locking granularity&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Consider two lookup implementations for global hash table:
&lt;code&gt;c
struct list *hash_tbl[1021];
&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;coarse-grained locking
&lt;code&gt;c
mutex_t m;
...
mutex_lock(&amp;amp;m);
struct list_elem
&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[OS Notes] Synchronization (Part II)</title>
      <link>/post/synchronization_2/</link>
      <pubDate>Mon, 01 Jan 2018 00:38:00 +0000</pubDate>
      
      <guid>/post/synchronization_2/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview-of-previous-and-current-lectures&#34;&gt;Overview of previous and current lectures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rcu-read-copy-update&#34;&gt;RCU (Read-copy update)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#is-rcu-really-safe&#34;&gt;Is RCU really safe?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#preemptible-kernels&#34;&gt;Preemptible kernels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#garbage-collection&#34;&gt;Garbage collection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improving-spinlock-performance&#34;&gt;Improving spinlock performance&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#useful-macros&#34;&gt;Useful macros&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mcs-lock&#34;&gt;MCS lock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mcs-acquire&#34;&gt;MCS Acquire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mcs-release-with-cas&#34;&gt;MCS Release with CAS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;overview-of-previous-and-current-lectures&#34;&gt;Overview of previous and current lectures&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Locks create serial code

&lt;ul&gt;
&lt;li&gt;Serial code gets no speedup from multiprocessor&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Test-and-set spinlock has additional disadvantages

&lt;ul&gt;
&lt;li&gt;Lots of traffic over memory bus&lt;/li&gt;
&lt;li&gt;Not fair on NUMA machines&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Idea 1: Avoid spinlocks

&lt;ul&gt;
&lt;li&gt;We saw lock-free algorithms last lecture&lt;/li&gt;
&lt;li&gt;Discussing RCU very quickly last time&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Idea 2: Design better spinlocks

&lt;ul&gt;
&lt;li&gt;Less memory traffic, better fairness&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Idea 3: Hardware turns coarse-into fine-grained locks!

&lt;ul&gt;
&lt;li&gt;While also reducing memory traffic for lock in common case&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;rcu-read-copy-update&#34;&gt;RCU (Read-copy update)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Some data is read way more often than written

&lt;ul&gt;
&lt;li&gt;Routing tables consulted for each forwarded packet&lt;/li&gt;
&lt;li&gt;Data maps in system with 100+ disks (updated on disk failure)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Optimize for the common case of reading without lock

&lt;ul&gt;
&lt;li&gt;E.g., global variable: &lt;code&gt;routing_table *rt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Call &lt;code&gt;lookup (rt, route);&lt;/code&gt; with no lock&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Update by making copy, swapping pointer
&lt;code&gt;C
routing_table *newrt = copy_routing_table (rt);
update_routing_table (newrt);
atomic_thread_fence (memory_order_release);
rt = newrt;
&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;is-rcu-really-safe&#34;&gt;Is RCU really safe?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Consider the use of global &lt;em&gt;rt&lt;/em&gt; with no fences: &lt;code&gt;loolup (rt, route);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Could a CPU read new pointer but then old contents of *rt?&lt;/li&gt;
&lt;li&gt;Yes on alpha, No on all other existing architectures&lt;/li&gt;
&lt;li&gt;We are saved by &lt;em&gt;dependency ordering&lt;/em&gt; in hardware

&lt;ul&gt;
&lt;li&gt;Instruction B depends on A if B uses result of A&lt;/li&gt;
&lt;li&gt;Non-alpha CPUs won&amp;rsquo;t re-order dependent instructions&lt;/li&gt;
&lt;li&gt;If wwriter uses release fence, safe to load pointer then just use it&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;This is the point of &lt;code&gt;memory_order_consume&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Should be quivalent to acquire barrier on alpha&lt;/li&gt;
&lt;li&gt;But should compile to nothing (be free) on other machines&lt;/li&gt;
&lt;li&gt;Active area of discussion for C++ committee&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;preemptible-kernels&#34;&gt;Preemptible kernels&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Recall &lt;em&gt;kernel process context&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;When CPU in kernel mode but excuting on behalf of a process (e.h., might be in system call or page fault handler)&lt;/li&gt;
&lt;li&gt;As opposed to interrupt handlers or context switch code&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A &lt;em&gt;preemptible kernel&lt;/em&gt; can preempt process context code

&lt;ul&gt;
&lt;li&gt;Take a CPU core away from kernel process context code between any two instructions&lt;/li&gt;
&lt;li&gt;Give the same CPU core to kernel code for a different process&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t confuse with

&lt;ul&gt;
&lt;li&gt;Interrupt handlers can always preempt process context code&lt;/li&gt;
&lt;li&gt;Preemptive threads (always have for multicore)&lt;/li&gt;
&lt;li&gt;Process context code running concurrently on other CPU cores&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Sometimes want or need to disable preemption

&lt;ul&gt;
&lt;li&gt;E.g., might help performance while holding a spinlock&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;garbage-collection&#34;&gt;Garbage collection&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;When can you free memory of old routing table?

&lt;ul&gt;
&lt;li&gt;When you are guaranteed no one is using it —— how to determine&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Definitions:

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;temporary variable&lt;/em&gt; - short-used (e.g., local) variable&lt;/li&gt;
&lt;li&gt;&lt;em&gt;permanent variable&lt;/em&gt; - long lived data (e.g., global &lt;em&gt;rt&lt;/em&gt; pointer)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;quiescent state&lt;/em&gt; - when all a thread&amp;rsquo;s temporary variables dead&lt;/li&gt;
&lt;li&gt;&lt;em&gt;quiescent period&lt;/em&gt; - time during which every thread has been in quiesceent state at least once&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Free old copy of updated data after quiescent period

&lt;ul&gt;
&lt;li&gt;How to determine when quiescent period has gone by?&lt;/li&gt;
&lt;li&gt;E.g., keep count of syscalls/context on each CPU&lt;/li&gt;
&lt;li&gt;Can&amp;rsquo;t hold a pointer across context switch or user mode&lt;/li&gt;
&lt;li&gt;Must disable preemption while consuming RCU data structure&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;improving-spinlock-performance&#34;&gt;Improving spinlock performance&lt;/h2&gt;

&lt;h3 id=&#34;useful-macros&#34;&gt;Useful macros&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Atomic compare and swap: CAS (mem, old, new)

&lt;ul&gt;
&lt;li&gt;In C11: &lt;code&gt;atomic_compare_exchange_strong&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;On x86: &lt;code&gt;cmpxchg&lt;/code&gt; instruction provides this (with &lt;em&gt;lock&lt;/em&gt; prefix)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;if *mem == old, then swap *mem&amp;lt;-&amp;gt;new and return true, else false&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Atomic swap: XCHG (mem, new)

&lt;ul&gt;
&lt;li&gt;C11 &lt;code&gt;atomic_exchange&lt;/code&gt;, can implement with xchg on x86&lt;/li&gt;
&lt;li&gt;Atomically exchanges &lt;code&gt;*mem&amp;lt;-&amp;gt;new&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Atomic fetch and add: FADD (mem, val)

&lt;ul&gt;
&lt;li&gt;C11 &lt;code&gt;atomic_fetch_add&lt;/code&gt;, can implement with &lt;em&gt;lock&lt;/em&gt; add on x86
Atomically sets &lt;code&gt;*mem += val&lt;/code&gt; and returns &lt;em&gt;old&lt;/em&gt; value of &lt;code&gt;*mem&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Atomic fetch and subtract: FSUB (mem, val)&lt;/li&gt;
&lt;li&gt;Note all atomics return previous value (like x++, not ++x)&lt;/li&gt;
&lt;li&gt;All behave like sequentially consistent fences, too

&lt;ul&gt;
&lt;li&gt;Unlike &lt;code&gt;_explicit&lt;/code&gt; versions, which take a &lt;code&gt;memory_order&lt;/code&gt; argument&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;mcs-lock&#34;&gt;MCS lock&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Idea 2: Build a better spinlock&lt;/li&gt;
&lt;li&gt;Lick designed by Mellor-Crummey and Scott

&lt;ul&gt;
&lt;li&gt;Goal: reduce bus traffic on cc machines, improve fairness&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Each CPU has a &lt;em&gt;qnode&lt;/em&gt; structure in local memory
&lt;code&gt;C
typedef struct qnode {
_Atomic (struct qnode *) next;
atomic_bool locked;
} qnode;
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A lock is a &lt;em&gt;qnode&lt;/em&gt; pointer: &lt;code&gt;typedef _Atomic (qnode *) lock;&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Construct lisk of CPUs holding or waiting for lock&lt;/li&gt;
&lt;li&gt;&lt;em&gt;lock&lt;/em&gt; itself points to taiil of list list&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;While waiting, spin on your local &lt;em&gt;locked&lt;/em&gt; flag&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;mcs-acquire&#34;&gt;MCS Acquire&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;acquire (lock *L, qnode*I){
    I-&amp;gt;next = NULL;
    qnode *preprocessor = I;
    XCHG (*L, predecessor);
    if (predecessor != NULL){
        I-&amp;gt;locked = true
        predecessor-&amp;gt;next = I;;
        while (I-&amp;gt;locked);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/QQ20180123-033259@2x.pnd&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If unlocked, L is NULL&lt;/li&gt;
&lt;li&gt;If locked, no waiters, L is owner&amp;rsquo;s qnode&lt;/li&gt;
&lt;li&gt;If waiters, *L is tail of waiter list&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;mcs-release-with-cas&#34;&gt;MCS Release with CAS&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;release (lock* L, qnode * I) {
    if (!I-&amp;gt;next)
        if (CAS (*L, I, NULL))
            return;
    while (!I-&amp;gt;nxt);
    I-&amp;gt;next-&amp;gt;locked false
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>[OS Notes] Virtual Memory OS</title>
      <link>/post/vm_os/</link>
      <pubDate>Wed, 27 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/vm_os/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#paging&#34;&gt;Paging&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#working-set-model&#34;&gt;Working set model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paging-challenge&#34;&gt;Paging challenge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#re-starting-instructions&#34;&gt;Re-starting instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-to-fetch&#34;&gt;What to fetch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selecting-physical-pages&#34;&gt;Selecting physical pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#superpages&#34;&gt;Superpages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eviction-policies&#34;&gt;Eviction policies&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#starw-man-fifo-eviction&#34;&gt;Starw man: FIFO eviction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thrashing&#34;&gt;Thrashing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#details-of-paging&#34;&gt;Details of paging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-user-level-perspective&#34;&gt;The user-level perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-4-4-bsd&#34;&gt;Case study: 4.4 BSD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;paging&#34;&gt;Paging&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171227-110059.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use disk to simulate larger virtual than physical memory&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;working-set-model&#34;&gt;Working set model&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Disk much, much slower than memory

&lt;ul&gt;
&lt;li&gt;Goal: run at memory speed, not disk speed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;sup&gt;80&lt;/sup&gt;&amp;frasl;&lt;sub&gt;20&lt;/sub&gt; rule: 20% of memory gets 80% of memory accesses

&lt;ul&gt;
&lt;li&gt;Keep the hot 20% in memory&lt;/li&gt;
&lt;li&gt;Keep the cold 80% on disk&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;paging-challenge&#34;&gt;Paging challenge&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;How to resume a process after a fault?

&lt;ul&gt;
&lt;li&gt;Need to save state and resume&lt;/li&gt;
&lt;li&gt;Process might have been in the middle of an instruction!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What to fetch from disk?

&lt;ul&gt;
&lt;li&gt;Just needed page or more?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What to eject?

&lt;ul&gt;
&lt;li&gt;How to allocate physical pages amongst processes?&lt;/li&gt;
&lt;li&gt;Wchich of a particular process&amp;rsquo;s pages to keep in memory?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;re-starting-instructions&#34;&gt;Re-starting instructions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Hardware provides kernel with information about page fault

&lt;ul&gt;
&lt;li&gt;Faulting virtual address (In &lt;code&gt;%cr2&lt;/code&gt; reg on x86&amp;ndash;may see it if you midify Pintos &lt;code&gt;page_fault&lt;/code&gt; and use &lt;code&gt;fault_addr&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Address of instruction that caused fault&lt;/li&gt;
&lt;li&gt;Was the access a read or write? Was it an instruction fetch? Was it caused by user accss to kernel-only memory?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Hardware mush allow resuming after a fault&lt;/li&gt;
&lt;li&gt;Idempotent instructions are easy

&lt;ul&gt;
&lt;li&gt;E.g., simple load or store instruction can be restarted&lt;/li&gt;
&lt;li&gt;Just re-execute any instruction that only accesses one address&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Complex instructions must be re-started, too

&lt;ul&gt;
&lt;li&gt;E.g., x86 move string instructions&lt;/li&gt;
&lt;li&gt;Specify src, dst, count in &lt;code&gt;%esi, %edi, %ecx&lt;/code&gt; registers&lt;/li&gt;
&lt;li&gt;On fault, registers adjusted to resume where move left off&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-to-fetch&#34;&gt;What to fetch&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Bring in page that caused page fault&lt;/li&gt;
&lt;li&gt;Pre-fetch surrounding pages?

&lt;ul&gt;
&lt;li&gt;Reading two disk blocks approximately as fast as reading one&lt;/li&gt;
&lt;li&gt;As long as no track/head switch, seek time dominates&lt;/li&gt;
&lt;li&gt;If application exhibits spacial locality, then big win to store and read multiple contiguous pages&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Also pre-zero unused pages in idle loop

&lt;ul&gt;
&lt;li&gt;Need 0-filled pages for stack, heap, anonymously mmapped memory&lt;/li&gt;
&lt;li&gt;Zeroing them only on demand is slower&lt;/li&gt;
&lt;li&gt;Hence, many OSes zero freed pages while CPU is idle&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;selecting-physical-pages&#34;&gt;Selecting physical pages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;May need to eject some pages

&lt;ul&gt;
&lt;li&gt;More on eviction policy in two slides&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;May also have a choice of physical pages&lt;/li&gt;
&lt;li&gt;Direct-mapped physical caches

&lt;ul&gt;
&lt;li&gt;Virtual -&amp;gt; Physical mapping can affect performance&lt;/li&gt;
&lt;li&gt;In old days: Physical adddredd A conflicts with $kC+A$ (where k is any integer, C is cache size)&lt;/li&gt;
&lt;li&gt;Applications can conflict with each other or themselves&lt;/li&gt;
&lt;li&gt;Scientific application benefit if consecutive virtual pages do not confilict in the cache&lt;/li&gt;
&lt;li&gt;Many other applications do better with random mapping&lt;/li&gt;
&lt;li&gt;There days: CPUs more sophisticated than kC + A&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;superpages&#34;&gt;Superpages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;How should OS make use of &amp;ldquo;large&amp;rdquo; mappings

&lt;ul&gt;
&lt;li&gt;x86 has &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt; MB pages that might be useful&lt;/li&gt;
&lt;li&gt;Alpha has even more choices: 8KB, 64KB, 512KB, 4MB&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Sometimes more pages in L2 cache than TLB entries

&lt;ul&gt;
&lt;li&gt;Don&amp;rsquo;t want costly TLB misses going to main memory&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Or have two-level TLBs

&lt;ul&gt;
&lt;li&gt;Want to maximize hit rate in faster L1 TLB&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OS can transparently support superpages

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;reserve&amp;rdquo; appropriate physical pages if possible&lt;/li&gt;
&lt;li&gt;Promote contiguous pages to superpages&lt;/li&gt;
&lt;li&gt;Does complicate evicting (esp. dirty pages) - demote&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;eviction-policies&#34;&gt;Eviction policies&lt;/h2&gt;

&lt;h3 id=&#34;starw-man-fifo-eviction&#34;&gt;Starw man: FIFO eviction&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Evict oldest fetched page in system&lt;/li&gt;
&lt;li&gt;Example-reference string 1,2,3,4,1,2,5,1,2,3,4,5&lt;/li&gt;
&lt;li&gt;3 physical pages: 9 page faults&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;thrashing&#34;&gt;Thrashing&lt;/h2&gt;

&lt;h2 id=&#34;details-of-paging&#34;&gt;Details of paging&lt;/h2&gt;

&lt;h2 id=&#34;the-user-level-perspective&#34;&gt;The user-level perspective&lt;/h2&gt;

&lt;h2 id=&#34;case-study-4-4-bsd&#34;&gt;Case study: 4.4 BSD&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>[OS Notes] Virtual Memory Hardware</title>
      <link>/post/vm_hardware/</link>
      <pubDate>Tue, 26 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/vm_hardware/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#issues-in-sharing-physical-memory&#34;&gt;Issues in sharing physical memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#virtual-memory&#34;&gt;Virtual memory&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#goals&#34;&gt;goals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#virtual-memory-advantages&#34;&gt;Virtual memory advantages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ideas&#34;&gt;Ideas&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ideal-1-load-time-linking&#34;&gt;Ideal 1: load-time linking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ideal-2-base-bound-register&#34;&gt;Ideal 2: base + bound register&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#definition&#34;&gt;Definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#base-bound&#34;&gt;Base + bound&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#trade-offs&#34;&gt;Trade-offs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#segmentation&#34;&gt;Segmentation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#segmentation-mechanics&#34;&gt;Segmentation mechanics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#segmentation-example&#34;&gt;Segmentation example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trade-offs-1&#34;&gt;Trade-offs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fragmentation&#34;&gt;Fragmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#alternatives-to-hardware-mmu&#34;&gt;Alternatives to hardware MMU&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paging&#34;&gt;Paging&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#trade-offs-2&#34;&gt;Trade-offs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simplified-allocation&#34;&gt;Simplified allocation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#paging-data-structures&#34;&gt;Paging data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-paging-on-pdp-11&#34;&gt;Example: Paging on PDP-11&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;issues-in-sharing-physical-memory&#34;&gt;Issues in sharing physical memory&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-152306.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Protection

&lt;ul&gt;
&lt;li&gt;A bug in one process can corrupt in another&lt;/li&gt;
&lt;li&gt;Must somehow prevent process A from trashing B&amp;rsquo;s memory&lt;/li&gt;
&lt;li&gt;Also prevent A from even observing B&amp;rsquo;s memory (ssh-agent)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Transparency

&lt;ul&gt;
&lt;li&gt;A process shouldn&amp;rsquo;t require particular physical memory bits&lt;/li&gt;
&lt;li&gt;Yet processes often require large amounts of contiguous memory (for stack, large data structures, etc.)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Resource exhaustion

&lt;ul&gt;
&lt;li&gt;Programmers typically assume machine has &amp;ldquo;enough&amp;rdquo; memory&lt;/li&gt;
&lt;li&gt;Sum of sizes of all processes often greater than physical memory&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;virtual-memory&#34;&gt;Virtual memory&lt;/h2&gt;

&lt;h3 id=&#34;goals&#34;&gt;goals&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Give each program its own &lt;em&gt;virtual&lt;/em&gt; address space

&lt;ul&gt;
&lt;li&gt;At runtime, &lt;strong&gt;Memory-Management Unit (MMU)&lt;/strong&gt; relocates each load/store&lt;/li&gt;
&lt;li&gt;Application doesn&amp;rsquo;t see physical memory addresses&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Also enforce protection

&lt;ul&gt;
&lt;li&gt;Prevent one app from messing with another&amp;rsquo;s memory&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And allow programs to see more memory than exists

&lt;ul&gt;
&lt;li&gt;Somehow relocate some memory accesses to disk&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;virtual-memory-advantages&#34;&gt;Virtual memory advantages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Can relocate program while running

&lt;ul&gt;
&lt;li&gt;Run partially in memory, partially on disk&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Most of a process&amp;rsquo;s memory may be idle (&lt;sup&gt;80&lt;/sup&gt;&amp;frasl;&lt;sub&gt;20&lt;/sub&gt; rule)

&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;/img/post/WX20171226-153927.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;li&gt;Write idle parts to disk until needed&lt;/li&gt;
&lt;li&gt;Let other processes use memory of idle part&lt;/li&gt;
&lt;li&gt;Like CPU virtualization: when process not using CPU, switch (Not using amemory region? switch it to another process)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Challenge: VM = extra layer, could be slow&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ideas&#34;&gt;Ideas&lt;/h2&gt;

&lt;h3 id=&#34;ideal-1-load-time-linking&#34;&gt;Ideal 1: load-time linking&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-154928.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Linker&lt;/em&gt; patches addresses of symbols like printf&lt;/li&gt;
&lt;li&gt;Idea: link when process executed, not at compile time

&lt;ul&gt;
&lt;li&gt;Determine where process will reside in memory&lt;/li&gt;
&lt;li&gt;Adjust all refernces within program (using addtion)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Problems?

&lt;ul&gt;
&lt;li&gt;How to enforce protection?&lt;/li&gt;
&lt;li&gt;How to move once already in memory? (consider data pointers)&lt;/li&gt;
&lt;li&gt;What if no contiguous free region fits program?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ideal-2-base-bound-register&#34;&gt;Ideal 2: base + bound register&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-163437.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Two special privileged registers: &lt;strong&gt;base&lt;/strong&gt; snf &lt;strong&gt;bound&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;On each load/store/jump:

&lt;ul&gt;
&lt;li&gt;Physical address = virtual address + base&lt;/li&gt;
&lt;li&gt;Check 0 ≤ vitual address ≤ bound, else trap to kernel&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;How to move process in memory?

&lt;ul&gt;
&lt;li&gt;Change base register&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What happens on context switch?

&lt;ul&gt;
&lt;li&gt;OS must re-load base and bound register&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;definition&#34;&gt;Definition&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Programs load/store to &lt;code&gt;virtual addresses&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Actual memory uses &lt;code&gt;physical addresses&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;VM Hardware is Memory Management Unit (&lt;code&gt;MMU&lt;/code&gt;)

&lt;ul&gt;
&lt;li&gt;Usually part of CPU&lt;/li&gt;
&lt;li&gt;Configured through privileged instructions (e.g., load bound reg)&lt;/li&gt;
&lt;li&gt;Translates from virtual to physical addresses&lt;/li&gt;
&lt;li&gt;Gives per-process view of memory called &lt;code&gt;address space&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;base-bound&#34;&gt;Base + bound&lt;/h2&gt;

&lt;h3 id=&#34;trade-offs&#34;&gt;Trade-offs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Advantages

&lt;ul&gt;
&lt;li&gt;Cheap in terms of hardware: only two registers&lt;/li&gt;
&lt;li&gt;Cheap in terms of cycles: do add and compare in parallel&lt;/li&gt;
&lt;li&gt;Examples: Cray-1 used this scheme&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disadvantages

&lt;ul&gt;
&lt;li&gt;Growing a process is expensive or impossible&lt;/li&gt;
&lt;li&gt;No way to share code or data (E.g., two copies of bochs, both running pintos)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;One solution: Multiple segments

&lt;ul&gt;
&lt;li&gt;E.g., separate code, stack, data segments&lt;/li&gt;
&lt;li&gt;Possibly multiple data segments&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;segmentation&#34;&gt;Segmentation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-164915.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Let processes have many base/bound regs

&lt;ul&gt;
&lt;li&gt;Address space built from many segments&lt;/li&gt;
&lt;li&gt;Can share/protect memory at segment granularity&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Must specify segment as part of virtual address&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;segmentation-mechanics&#34;&gt;Segmentation mechanics&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-191449.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each process has a segment table&lt;/li&gt;
&lt;li&gt;Each VA (Virtual Address) indicates a segment and offset:

&lt;ul&gt;
&lt;li&gt;Top bits of addr select segment, low bits select offset (PDP-10)&lt;/li&gt;
&lt;li&gt;Or segment selected by instruction or operand (means you need wider &amp;ldquo;far&amp;rdquo; pointers to specify segment)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;segmentation-example&#34;&gt;Segmentation example&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/post/WX20171226-192145.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2-bit segment number (1st digit), 12 bit offset (last 3)

&lt;ul&gt;
&lt;li&gt;Where is 0x0&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;trade-offs-1&#34;&gt;Trade-offs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Advantages

&lt;ul&gt;
&lt;li&gt;Multiple segments per process&lt;/li&gt;
&lt;li&gt;Allows sharing!&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t need entire process in memory&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disadvantages

&lt;ul&gt;
&lt;li&gt;Requires translation hardware, which could limit performance&lt;/li&gt;
&lt;li&gt;Segments not completely transparent to program (e.g., default segment faster or uses shorter instruction)&lt;/li&gt;
&lt;li&gt;$n$ byte segment needs &lt;em&gt;n contiguous&lt;/em&gt; bytes of physical memory&lt;/li&gt;
&lt;li&gt;Makes &lt;em&gt;fragmentation&lt;/em&gt; a real problem&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;fragmentation&#34;&gt;Fragmentation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Fragmentation =&amp;gt; Inability to use free memory&lt;/li&gt;
&lt;li&gt;Over time:

&lt;ul&gt;
&lt;li&gt;Variable-sized pices = many small holes (external fragmentation)&lt;/li&gt;
&lt;li&gt;Fixed-sized pieces = no external holes, but force internal waste (internal fragmentation)
&lt;img src=&#34;/img/post/WX20171226-213655.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;alternatives-to-hardware-mmu&#34;&gt;Alternatives to hardware MMU&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Language-level protection (Java)

&lt;ul&gt;
&lt;li&gt;Single address space for different modules&lt;/li&gt;
&lt;li&gt;Language enforces isolation&lt;/li&gt;
&lt;li&gt;Singularity OS does this&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Software fault isolation

&lt;ul&gt;
&lt;li&gt;Instrument compiler output&lt;/li&gt;
&lt;li&gt;Checks before ever ystore operation prevents modules from trashing each other&lt;/li&gt;
&lt;li&gt;Google Native Client does this&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;paging&#34;&gt;Paging&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Divede memory up into small &lt;em&gt;pages&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Map virtual pages to physical pages

&lt;ul&gt;
&lt;li&gt;Each process has separate mapping&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Allow OS to gain control on certain operation

&lt;ul&gt;
&lt;li&gt;Read-only pages trap to OS on write&lt;/li&gt;
&lt;li&gt;Invalid pages trap to OS on read or write&lt;/li&gt;
&lt;li&gt;OS can change mapping and resume application&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Other features sometimes found:

&lt;ul&gt;
&lt;li&gt;Hardware can set &amp;ldquo;accessed&amp;rdquo; and &amp;ldquo;dirty&amp;rdquo; bits&lt;/li&gt;
&lt;li&gt;Control page execute permission separately from read/write&lt;/li&gt;
&lt;li&gt;Control caching or memory consistency of page&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;trade-offs-2&#34;&gt;Trade-offs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Eliminates external fragmentation&lt;/li&gt;
&lt;li&gt;Simplifies allocation, free, and backing storage (swap)&lt;/li&gt;
&lt;li&gt;Average internal fragmentation of .5 pages per &amp;ldquo;segment&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;simplified-allocation&#34;&gt;Simplified allocation&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Allocate any physical page to any process&lt;/li&gt;
&lt;li&gt;Can store idle virtual pages on disk&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;paging-data-structures&#34;&gt;Paging data structures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Pages are fixed size, e.g., 4K

&lt;ul&gt;
&lt;li&gt;Least significant 12 ($log_{2}4K$) bits of address are page offset&lt;/li&gt;
&lt;li&gt;Most significant bits are &lt;em&gt;page number&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Each process has a page table

&lt;ul&gt;
&lt;li&gt;Maps &lt;em&gt;virtual page numbers (VPNs)&lt;/em&gt; to &lt;em&gt;physical page numbers (PPNS)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Also includes bits for protection, validity, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;On memory access: Translate VPN to PPN, then add offset
&lt;img src=&#34;/img/post/WX20171227-094654.png&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;example-paging-on-pdp-11&#34;&gt;Example: Paging on PDP-11&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;64K virtual memory, 8K pages

&lt;ul&gt;
&lt;li&gt;Separate address space for instructions &amp;amp; data&lt;/li&gt;
&lt;li&gt;I.e., can&amp;rsquo;t read your own instructions with a load&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Entire page table stored in registers

&lt;ul&gt;
&lt;li&gt;8 Instruction page translation&lt;/li&gt;
&lt;li&gt;8 Data page translation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Swap 16 machine registers on each context switch&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-12</title>
      <link>/post/til-2017-12-12/</link>
      <pubDate>Tue, 12 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-12/</guid>
      <description>

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note-deep-learning-practice-and-trends-nips-2017&#34;&gt;[Note] Deep Learning: Practice and Trends (NIPS 2017)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#practice&#34;&gt;Practice&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architectures&#34;&gt;Architectures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recurrent-nets&#34;&gt;Recurrent Nets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trends&#34;&gt;Trends&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#autoregressive-models&#34;&gt;Autoregressive Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#domain-alignment-unsupervised&#34;&gt;Domain Alignment (unsupervised)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learning-to-learn-meta-learning&#34;&gt;Learning to Learn / Meta Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions-and-expectations&#34;&gt;Conclusions and Expectations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h1 id=&#34;note-deep-learning-practice-and-trends-nips-2017&#34;&gt;[Note] Deep Learning: Practice and Trends (NIPS 2017)&lt;/h1&gt;

&lt;h2 id=&#34;practice&#34;&gt;Practice&lt;/h2&gt;

&lt;h3 id=&#34;architectures&#34;&gt;Architectures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Convolutional Nets

&lt;ul&gt;
&lt;li&gt;Locality: objects tend to have a local spatial support

&lt;ul&gt;
&lt;li&gt;locally-connected&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Teanslation invariance: object appearance is independent of location&lt;/li&gt;
&lt;li&gt;Tricks of the Trade

&lt;ul&gt;
&lt;li&gt;Optimization

&lt;ul&gt;
&lt;li&gt;SGD with momentum&lt;/li&gt;
&lt;li&gt;Batch Norm&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Initialization

&lt;ul&gt;
&lt;li&gt;Weight init: start from the weights which lead to stable training&lt;/li&gt;
&lt;li&gt;Sample from zero-mean normal distribution w/ small variance 0.01

&lt;ul&gt;
&lt;li&gt;Adaptively choose variance for each layer

&lt;ul&gt;
&lt;li&gt;preserve gradient magnitude: 1/sqrt(fan_in)&lt;/li&gt;
&lt;li&gt;works fine for VGGNets (up to 20 layers), but not sufficient for deeper nets&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Model

&lt;ul&gt;
&lt;li&gt;Stacking 3x3 convolutions&lt;/li&gt;
&lt;li&gt;Inception&lt;/li&gt;
&lt;li&gt;ResNet adds modules which ensure that the gradient doesn&amp;rsquo;t vanish&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;recurrent-nets&#34;&gt;Recurrent Nets&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Two Key Ingredients: Neural Embeddings, Recurrent Language Models&lt;/li&gt;
&lt;li&gt;Dot product Attention

&lt;ul&gt;
&lt;li&gt;Inputs: &amp;ldquo;I am a cat&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Input RNN states: $e_1e_2e_3e_4$&lt;/li&gt;
&lt;li&gt;Decoder RNN state at step i (query): $h_i$&lt;/li&gt;
&lt;li&gt;Compute scalars $h_i^Te_1, h_i^Te_2,&amp;hellip;$representing similarity / relevance between encoder steps and query&lt;/li&gt;
&lt;li&gt;Normaliza $[h_i^Te_1,h_i^Te_2,&amp;hellip;]$ with softmax to produce attention weights&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tricks of the Trade

&lt;ul&gt;
&lt;li&gt;Long sequences?

&lt;ul&gt;
&lt;li&gt;Attention&lt;/li&gt;
&lt;li&gt;Bigger state&lt;/li&gt;
&lt;li&gt;Reverse inputs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Can&amp;rsquo;t overfit?

&lt;ul&gt;
&lt;li&gt;Bigger hidden state&lt;/li&gt;
&lt;li&gt;Deep LSTM + Skip Connections&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Overfit?

&lt;ul&gt;
&lt;li&gt;Dropout + Ensembles&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tuning

&lt;ul&gt;
&lt;li&gt;Keep calm and decrease your learning rate&lt;/li&gt;
&lt;li&gt;Initalization of parameters is critical (in seq2seq we used U(-0.05, 0.05))&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clip the gradients!&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;E.g. if ||grad|| &amp;gt; 5: grad = grad/||grad|| * 5&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Attention and Memory Toolbox

&lt;ul&gt;
&lt;li&gt;Read/Write memories (neural turing machine)&lt;/li&gt;
&lt;li&gt;Sequence Prediction&lt;/li&gt;
&lt;li&gt;Seq2Seq&lt;/li&gt;
&lt;li&gt;Temporal Hierarchies&lt;/li&gt;
&lt;li&gt;Multimodality&lt;/li&gt;
&lt;li&gt;Attention/Pointers&lt;/li&gt;
&lt;li&gt;Recurrent Architectures&lt;/li&gt;
&lt;li&gt;Key, Value memories&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;trends&#34;&gt;Trends&lt;/h2&gt;

&lt;h3 id=&#34;autoregressive-models&#34;&gt;Autoregressive Models&lt;/h3&gt;

&lt;p&gt;$$P(x;\theta) = \prod_{n=1}^N P(x_n|x_n;\theta)$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each factor can be parametrized by $\theta$, which can be shared&lt;/li&gt;
&lt;li&gt;The variables can be arbitrarily ordered and grouped, as long as the ordering and grouping is consistent.&lt;/li&gt;
&lt;li&gt;Building Blocks

&lt;ul&gt;
&lt;li&gt;Inputs and Outpus: these can also be conditioning variables

&lt;ul&gt;
&lt;li&gt;Image pixels&lt;/li&gt;
&lt;li&gt;Text sequences&lt;/li&gt;
&lt;li&gt;Audio waveforms&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Architectures

&lt;ul&gt;
&lt;li&gt;Recurrent, over space and time&lt;/li&gt;
&lt;li&gt;Causal convolutions&lt;/li&gt;
&lt;li&gt;Causal conv + attention&lt;/li&gt;
&lt;li&gt;Attention-only!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Losses:

&lt;ul&gt;
&lt;li&gt;Discrete case: softmax cross entropy&lt;/li&gt;
&lt;li&gt;Continuous: Gaussian (mixture) likelihood&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mixture of logistics loss&lt;/strong&gt; (PixelCNN++ in ICLR 2017)

&lt;ul&gt;
&lt;li&gt;$$ v ~ \sum_{i=1}^K \pi_i logistic(\mu_i,s_i) $$&lt;/li&gt;
&lt;li&gt;$$ P(x|\pi,\mu,s) = \sum_{i=1}^K \pi_i[\sigma((x+0.5-\mu_i)/s_i) - \sigma((x-0.5-\mu_i) / s_i)] $$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Autogressive scoring and sampling

&lt;ul&gt;
&lt;li&gt;Fully sequential models:

&lt;ul&gt;
&lt;li&gt;PixelCNN, PixelCNN++, WaveNet, &amp;hellip;&lt;/li&gt;
&lt;li&gt;O(1) scoring, O(N) sampling&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Models with conditional independence assumptions:

&lt;ul&gt;
&lt;li&gt;O(1) scoring, sampling can be O(1), O(log N), etc depending on cond. indep. assumptions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Distilled models:

&lt;ul&gt;
&lt;li&gt;Parall WaveNet, Parallel NMT&lt;/li&gt;
&lt;li&gt;O(N) scoring, O(1) sampling&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;domain-alignment-unsupervised&#34;&gt;Domain Alignment (unsupervised)&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Building Blocks

&lt;ul&gt;
&lt;li&gt;Inputs and Outpus

&lt;ul&gt;
&lt;li&gt;Sets of images with shared structure, but weak or no alignment&lt;/li&gt;
&lt;li&gt;Text corpora in different languages, but not parallel&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Architecures

&lt;ul&gt;
&lt;li&gt;Nothing fancy!&lt;/li&gt;
&lt;li&gt;For images: mostly convolutional nets&lt;/li&gt;
&lt;li&gt;For text: recurrent&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Losses

&lt;ul&gt;
&lt;li&gt;Latent space: Domain confusion&lt;/li&gt;
&lt;li&gt;Pixel space: Cycle consistency&lt;/li&gt;
&lt;li&gt;Both adversarial loss and likelihoods work!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Approach

&lt;ul&gt;
&lt;li&gt;Cross-modal retrieval&lt;/li&gt;
&lt;li&gt;Unsupervised domain transfer for classification&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Case

&lt;ul&gt;
&lt;li&gt;DiscoGAN - Car2Face&lt;/li&gt;
&lt;li&gt;GraspGAN&lt;/li&gt;
&lt;li&gt;Unsupervised Neural Machine Translation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;learning-to-learn-meta-learning&#34;&gt;Learning to Learn / Meta Learning&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Building Blocks:

&lt;ul&gt;
&lt;li&gt;Inputs and Outputs: text, images, actions&lt;/li&gt;
&lt;li&gt;Architectures: Recurrent, CNN (+attention)&lt;/li&gt;
&lt;li&gt;Losses (loss based on another loss): Model, Optimization, Initialization&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Learning to learn:

&lt;ul&gt;
&lt;li&gt;What is Meta Learning?

&lt;ul&gt;
&lt;li&gt;Go beyond train/test from same distribution&lt;/li&gt;
&lt;li&gt;Task between train/test changes, so model has to &amp;ldquo;learn to learn&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Model Based, Metric Based, Optimization Based&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;conclusions-and-expectations&#34;&gt;Conclusions and Expectations&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Deep autoregressive models and ConvNets are ubiquitous and already useful in consumer applications&lt;/li&gt;
&lt;li&gt;Inductive biases are useful

&lt;ul&gt;
&lt;li&gt;spatial invariance for CNNs&lt;/li&gt;
&lt;li&gt;time recurrence for RNNs&lt;/li&gt;
&lt;li&gt;Permutation invariance for Graphs&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Simple tricks like ResNet will be discovered&lt;/li&gt;
&lt;li&gt;Adversarial networks and unsupervised domain adaptation have interesting market app (e.g. phone apps like style transfer)&lt;/li&gt;
&lt;li&gt;Meta learning: more and more of the model lifecycle (train/val/test) will be learned in an end-to-end way&lt;/li&gt;
&lt;li&gt;Program syntesis + Graph networks may be very important and find more real-world applications (e.g. RobustFill)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-08</title>
      <link>/post/til-2017-12-08/</link>
      <pubDate>Fri, 08 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-08/</guid>
      <description>

&lt;h2 id=&#34;paper-daily&#34;&gt;Paper Daily&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Where Classification Fails, Interpretation Rises

&lt;ul&gt;
&lt;li&gt;apply an attention mechanism to the adversarial examples detection&lt;/li&gt;
&lt;li&gt;uisng attention to defend against adversarial examples&lt;/li&gt;
&lt;li&gt;(this paper is hard to read)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deep Image Prior

&lt;ul&gt;
&lt;li&gt;randomly-initialized neural network can be used as a handcrafted prior with excellent results&lt;/li&gt;
&lt;li&gt;search in parameter space&lt;/li&gt;
&lt;li&gt;apply untrianed CNN, fit a G network to a single degraded image.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deep Reinforcement Learning framework for Autonoumous Driving

&lt;ul&gt;
&lt;li&gt;Deep Deterministic Actor Critic (DDAC)

&lt;ul&gt;
&lt;li&gt;actor: provides the policy mapping from a state to action&lt;/li&gt;
&lt;li&gt;critic: evaluates the value of the action taken (same as Q-function)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;review-of-clustering-algorithms&#34;&gt;Review of clustering algorithms&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Clustering

&lt;ul&gt;
&lt;li&gt;k-means&lt;/li&gt;
&lt;li&gt;k-medoids&lt;/li&gt;
&lt;li&gt;Gaussian Mixture Model&lt;/li&gt;
&lt;li&gt;Spectral Clustering&lt;/li&gt;
&lt;li&gt;Hierachical Clustering&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Other relevant algorithms

&lt;ul&gt;
&lt;li&gt;Expecatation Maximization&lt;/li&gt;
&lt;li&gt;Dimendsionality Reduction

&lt;ul&gt;
&lt;li&gt;Laplacian Eigenmap&lt;/li&gt;
&lt;li&gt;Locally Linear Embedding&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;computer-orgnisation&#34;&gt;Computer orgnisation&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Direct mapping

&lt;ul&gt;
&lt;li&gt;process is divided into pages&lt;/li&gt;
&lt;li&gt;main memory is divided into frames/blocks&lt;/li&gt;
&lt;li&gt;cache is divided into lines&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2017-12-05</title>
      <link>/post/til-2017-12-05/</link>
      <pubDate>Tue, 05 Dec 2017 00:38:00 +0000</pubDate>
      
      <guid>/post/til-2017-12-05/</guid>
      <description>

&lt;h2 id=&#34;paper-daily&#34;&gt;Paper Daily&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;(NIPS 2017) Visual Reference Resolution using Attention Memory for Visual Dialog, Paul Hongsuch Seo

&lt;ul&gt;
&lt;li&gt;An associative attention memory storing s sequence of previous (attention, key) pairs&lt;/li&gt;
&lt;li&gt;Retrieves the previous attention, taking into account recency, which is most relevant for the current Q&lt;/li&gt;
&lt;li&gt;Resoning ability, maybe the best single model (trained by mle) presently&lt;/li&gt;
&lt;li&gt;Other keywords, text as key, attention as value, aceess values according to key; Method is Attention over attention&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;computer-network-notes&#34;&gt;Computer Network Notes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Classes (think in binary system)

&lt;ul&gt;
&lt;li&gt;CA: 1-126 ($2^{24}-2$)&lt;/li&gt;
&lt;li&gt;CB: 128-191 ($2^{16}-2$)&lt;/li&gt;
&lt;li&gt;CC: 192-223 ($2^8-2$)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;DBA: Directed Broadcast Address (NID, HID 1s [11.255.255.255])&lt;/li&gt;
&lt;li&gt;NID: (NID, HID 0s [11.0.0.0])&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;types-of-casting&#34;&gt;Types of casting&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Unicast&lt;/li&gt;
&lt;li&gt;Broadcast

&lt;ul&gt;
&lt;li&gt;Limited

&lt;ul&gt;
&lt;li&gt;11.0.0.0  |m|11.1.2.3 (Source Address)|255.255.255.255 (Destination Address)|&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Directed

&lt;ul&gt;
&lt;li&gt;11.0.0.0  |m|11.1.2.3|20.255.255.255|&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;subnets-subnet-mask-cidr&#34;&gt;Subnets, Subnet Mask, CIDR&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;subnetting disadvantage

&lt;ul&gt;
&lt;li&gt;reach the process complexly&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The difference between in subnet and outside subnet;&lt;/li&gt;
&lt;li&gt;Every subnet waste two IP addresses for the NID and DBA

&lt;ul&gt;
&lt;li&gt;2 subnet: For CC: (128 - 2) x 2 = 252&lt;/li&gt;
&lt;li&gt;2 subnet: (200.1.2.0)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Subnet Mask: 32 bit

&lt;ul&gt;
&lt;li&gt;#1: #NID + #SID, #0: #HID&lt;/li&gt;
&lt;li&gt;can find out NID&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Variable lenght subnet masking (VLSM)&lt;/li&gt;
&lt;li&gt;Classless Inter Domain Routing

&lt;ul&gt;
&lt;li&gt;(a.b.c.d/n) n: #NID&lt;/li&gt;
&lt;li&gt;Rules of blocks

&lt;ul&gt;
&lt;li&gt;All IP addrees should be contiguous&lt;/li&gt;
&lt;li&gt;$2^n$&lt;/li&gt;
&lt;li&gt;Fast IP address in the block should be evenly divided by size of the block&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Subnetting in CIDR&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;operating-system-notes&#34;&gt;Operating System Notes&lt;/h2&gt;

&lt;h2 id=&#34;file-system&#34;&gt;File System&lt;/h2&gt;

&lt;h3 id=&#34;cache-memory&#34;&gt;cache memory&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Cache - Paging (main memory) - Secondary Memory

&lt;ul&gt;
&lt;li&gt;Hit latency: the time to hit in the cache&lt;/li&gt;
&lt;li&gt;Cache hit: a state in which data requested for processing by a component or application is found in the cache memory&lt;/li&gt;
&lt;li&gt;Cache miss: not found&lt;/li&gt;
&lt;li&gt;Miss latency: the time (in cycles) the CPU waits when a miss happen in the cache&lt;/li&gt;
&lt;li&gt;Page fault, Page hit&lt;/li&gt;
&lt;li&gt;Spatial/temporal locality&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Direct Mapping

&lt;ul&gt;
&lt;li&gt;[Tag | Index (line number)| (block) Offset]&lt;/li&gt;
&lt;li&gt;$2^m$ addresses&lt;/li&gt;
&lt;li&gt;$2^k$ cache entries&lt;/li&gt;
&lt;li&gt;$2^n$ block size&lt;/li&gt;
&lt;li&gt;Step:

&lt;ul&gt;
&lt;li&gt;Use the index part of the address to find the appropriate cache entry&lt;/li&gt;
&lt;li&gt;CHeck the tag to see if the entry contains the right data&lt;/li&gt;
&lt;li&gt;If it does, then use the offset to the find the correct byte&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to Imitation Learning (Part 1)</title>
      <link>/post/imitation-learning-1/</link>
      <pubDate>Sun, 12 Mar 2017 22:38:00 +0000</pubDate>
      
      <guid>/post/imitation-learning-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Knowledge arrangement about Reinforcement Learning</title>
      <link>/post/knowledge-arrangement-about-rl/</link>
      <pubDate>Wed, 20 Apr 2016 11:00:00 +0000</pubDate>
      
      <guid>/post/knowledge-arrangement-about-rl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The knowledge of domain name that everyone should know</title>
      <link>/post/knowledge-of-domain-name/</link>
      <pubDate>Mon, 11 Jan 2016 07:45:00 +0000</pubDate>
      
      <guid>/post/knowledge-of-domain-name/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The construction of user portrait based on Big Data (Theoretical section)</title>
      <link>/post/user-portrait-theoretical/</link>
      <pubDate>Sun, 10 Jan 2016 05:58:00 +0000</pubDate>
      
      <guid>/post/user-portrait-theoretical/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
